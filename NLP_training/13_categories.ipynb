{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['POLITICS', 'TRAVEL', 'HEALTHY LIVING', 'BUSINESS', 'BLACK VOICES', 'THE WORLDPOST', 'IMPACT', 'MEDIA', 'WORLDPOST', 'SCIENCE', 'TECH', 'FIFTY', 'ENVIRONMENT', 'CULTURE & ARTS']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\אבינעם\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\אבינעם\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\אבינעם\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import nltk\n",
    "import sklearn\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import re\n",
    "import torch.utils.data as torch_data\n",
    "import scipy\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "# from Utils.pytorch_utils import torch_net\n",
    "from Utils.pytorch_utils import sparse_to_matrix #, accuracy_test\n",
    "\n",
    "from Utils.NLP_utils import accuracy, find_senteces_with_lemma, get_wordnet_pos, load_and_lemmatize_data, load_processed_data\n",
    "\n",
    "# pickle file, data set as readable json file, since original data set is a 'pseudo json', written in text file.\n",
    "DATA_SET_FILE = r\"C:\\Users\\גורים\\PycharmProjects\\NLP_training\\datasets\\News_Category_Dataset_v2_mod.pkl\"\n",
    "PROCESSED_DATA_SET = r\"C:\\Users\\גורים\\PycharmProjects\\NLP_training\\datasets\\News_Category_Dataset_v2_mod_processed.pkl\"\n",
    "\n",
    "ALL_CATEGORIES = ['POLITICS', 'WELLNESS', 'ENTERTAINMENT', 'TRAVEL', 'STYLE & BEAUTY',\n",
    "       'PARENTING', 'HEALTHY LIVING', 'QUEER VOICES', 'FOOD & DRINK',\n",
    "       'BUSINESS', 'COMEDY', 'SPORTS', 'BLACK VOICES', 'HOME & LIVING',\n",
    "       'PARENTS', 'THE WORLDPOST', 'WEDDINGS', 'WOMEN', 'IMPACT', 'DIVORCE',\n",
    "       'CRIME', 'MEDIA', 'WEIRD NEWS', 'GREEN', 'WORLDPOST', 'RELIGION',\n",
    "       'STYLE', 'SCIENCE', 'WORLD NEWS', 'TASTE', 'TECH', 'MONEY', 'ARTS',\n",
    "       'FIFTY', 'GOOD NEWS', 'ARTS & CULTURE', 'ENVIRONMENT', 'COLLEGE',\n",
    "       'LATINO VOICES', 'CULTURE & ARTS', 'EDUCATION']\n",
    "\n",
    "# REQUIRED_CATEGORIES = ['RELIGION','SCIENCE', 'TASTE','PARENTING' , 'COLLEGE' ,'POLITICS' ]\n",
    "REQUIRED_CATEGORIES = (np.array(ALL_CATEGORIES)[::3]).tolist()\n",
    "print (REQUIRED_CATEGORIES)\n",
    "NUM_CATEGORIES = len(REQUIRED_CATEGORIES)\n",
    "    \n",
    "    \n",
    "CrossEntropyLoss = \"CrossEntropyLoss\"\n",
    "MSELoss = \"MSELoss\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# set loss type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_name = CrossEntropyLoss\n",
    "# loss_name = MSELoss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sparse_to_matrix(A):\n",
    "    if type(A) == scipy.sparse.csr.csr_matrix:\n",
    "        return np.array(A.todense())\n",
    "    return A\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def confusion_matrix_disp(y_pred,y_true,label = False):\n",
    "    cm = confusion_matrix(y_pred,y_true)\n",
    "    if not label:\n",
    "        label = range(NUM_CATEGORIES)\n",
    "    cm_pd = pd.DataFrame(cm,index = [\"{}_P\".format(i)  for i in label],columns = [\"{}_T\".format(i)  for i in label])\n",
    "    return cm_pd\n",
    "\n",
    "# def accuracy_test_dummies(model, x, y, data_set_name = 'test',print_sample = False):\n",
    "#     predicted = torch.argmax(model(torch.tensor(x, dtype=torch.float)), dim=-1).numpy()\n",
    "#     truth = np.argmax(y, axis=-1)\n",
    "#     # print(np.array((predicted, truth)))\n",
    "#     print (f\"Accuracy {data_set_name} = \",\n",
    "#            round( np.array(predicted == truth).mean()* 100, 3 ),\n",
    "#            \"%\")\n",
    "#     if print_sample:\n",
    "#         ps = print_sample\n",
    "#         sample = pd.DataFrame((predicted[:ps],truth[:ps]),index=['predicted','truth'])\n",
    "#         print (sample)\n",
    "#         print (pd.value_counts(predicted))\n",
    "#         print (\"\\tConfusion Matrix:\\n\",confusion_matrix_disp (predicted,truth))\n",
    "        \n",
    "# def accuracy_test_classes(model, x, y, data_set_name = 'test',print_sample = False):\n",
    "#     predicted = torch.argmax(model(torch.tensor(x, dtype=torch.float)), dim=-1).numpy()\n",
    "#     truth = y.ravel()\n",
    "#     # print(np.array((predicted, truth)))\n",
    "#     print (f\"Accuracy {data_set_name} = \",\n",
    "#            round( np.array(predicted == truth).mean()* 100, 3 ),\n",
    "#            \"%\")\n",
    "#     if print_sample:\n",
    "#         ps = print_sample\n",
    "#         sample = pd.DataFrame((predicted[:ps],truth[:ps]),index=['predicted','truth'])\n",
    "#         print (sample)\n",
    "#         print (pd.value_counts(predicted))\n",
    "#         print (\"\\tConfusion Matrix:\\n\",confusion_matrix_disp (predicted,truth))\n",
    "\n",
    "def accuracy_test(model, x, y, data_set_name = 'test',print_sample = False,top_n_guess = 1):\n",
    "          \n",
    "    if loss_name == MSELoss:\n",
    "        truth = np.argmax(y, axis=-1)\n",
    "    elif loss_name == CrossEntropyLoss:\n",
    "        truth = y.ravel()\n",
    "    \n",
    "    model_result = model(torch.tensor(x, dtype=torch.float))\n",
    "    top_guesses = torch.argsort( model_result , dim = -1, descending=True ).numpy() [:,:top_n_guess]\n",
    "    # predicted   = torch.argmax ( model_result, dim=-1).numpy()\n",
    "    true_predicted = (top_guesses == truth[:,None]).any(1)\n",
    "    \n",
    "#     print (\"model_result = \", model_result[:10])\n",
    "#     print (\"top_guesses =\", top_guesses[:10])\n",
    "#     print (\"predicted = \", predicted[:10])\n",
    "\n",
    "    print (f\"Accuracy (top {top_n_guess} guesses) - {data_set_name} = \",\n",
    "           round( np.array(true_predicted).mean()* 100, 3 ),\n",
    "           \"%\")\n",
    "    if print_sample:\n",
    "        ps = print_sample\n",
    "        sample = pd.DataFrame((top_guesses[:ps],truth[:ps]),index=['predicted','truth'])\n",
    "        print (sample)\n",
    "        # print (pd.value_counts(predicted))\n",
    "    print (f\"\\tConfusion Matrix {data_set_name}:\\n\",confusion_matrix_disp (top_guesses[:,0],truth))\n",
    "    \n",
    "    \n",
    "        \n",
    "# if loss_name == MSELoss:\n",
    "#     accuracy_test = accuracy_test_dummies\n",
    "# elif loss_name == CrossEntropyLoss:\n",
    "#     accuracy_test = accuracy_test_classes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading data\n",
    "\n",
    "# dataset, headlines, headlines_orig = load_and_lemmatize_data(DATA_SET_FILE)\n",
    "dataset, headlines, headlines_orig = load_processed_data(PROCESSED_DATA_SET)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# reduce dataset to n categories:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = dataset['category']\n",
    "pd.value_counts(categories)\n",
    "\n",
    "# filter data for desired categories, to make problem easier\n",
    "filter_categories = True\n",
    "if filter_categories:\n",
    "    filter_index =  categories.isin(REQUIRED_CATEGORIES)\n",
    "    dataset   = dataset[filter_index]\n",
    "    headlines = np.array(headlines)[filter_index]\n",
    "    headlines_orig = np.array(headlines_orig)[filter_index]\n",
    "    \n",
    "else:\n",
    "    NUM_CATEGORIES = len(set(categories))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def categories_to_index(categories):\n",
    "    d = {}\n",
    "    for i, cat in enumerate(REQUIRED_CATEGORIES):\n",
    "        d[cat] = i\n",
    "        \n",
    "    r = np.array(range(len(categories)))\n",
    "\n",
    "    for cat,i in d.items():\n",
    "        # print (cat,i)\n",
    "        r[categories == cat ] = i\n",
    "    return r\n",
    "\n",
    "categories = dataset['category']\n",
    "if loss_name == CrossEntropyLoss:  \n",
    "    Y = categories_to_index(categories)[:,np.newaxis]\n",
    "else:\n",
    "    Y  = np.array(pd.get_dummies(categories)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data and lables to train/test\n",
    "\n",
    "headlines_train, headlines_test,\\\n",
    "headlines_train_orig, headlines_test_orig,\\\n",
    "Y_train, Y_test,\\\n",
    "    = sklearn.model_selection.train_test_split(\n",
    "    headlines,headlines_orig, Y, test_size = 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract features (Bag Of Words) using Vectorizer\n",
    "\n",
    "max_features=2000\n",
    "\n",
    "vectorizer = CountVectorizer\n",
    "# vectorizer = TfidfVectorizer\n",
    "matrix = vectorizer(max_features=max_features, ngram_range=(1, 2), max_df=0.1 ,min_df = 5)\n",
    "matrix.fit(headlines_train)\n",
    "X_train = matrix.transform(headlines_train)# .todense()\n",
    "X_test = matrix.transform(headlines_test)# .todense()\n",
    "\n",
    "# --- convert to data frame for display and debug ---\n",
    "# tokens = matrix.get_feature_names()\n",
    "# X_train= pd.DataFrame(X_train,columns=tokens)\n",
    "# X_test= pd.DataFrame(X_test,columns=tokens)\n",
    "\n",
    "assert X_train.shape[1]==max_features, X_train.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "updated torch nn\n"
     ]
    }
   ],
   "source": [
    "def torch_net(X_train, Y_train, X_test, Y_test,\n",
    "              hidden_layers=[10], device=torch.device('cpu'), epoch=30, batch_size=17):\n",
    "    \n",
    "    def set_learning_rate(optimizer,lr):\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = lr\n",
    "            \n",
    "    def print_learning_rate(optimizer):\n",
    "        for param_group in optimizer.param_groups:\n",
    "            print('lr', param_group['lr'])\n",
    "    \n",
    "    # hiden_layers = [size1,size2...]\n",
    "\n",
    "        \n",
    "    dtype = torch.float\n",
    "    # device = torch.device(\"cuda:0\") # Uncomment this to run on GPU\n",
    "\n",
    "    # N is batch size; D_in is input dimension;\n",
    "    # H is hidden dimension; D_out is output dimension.\n",
    "    N, D_in = X_train.shape\n",
    "    \n",
    "    if loss_name == MSELoss: \n",
    "        D_out = Y_train.shape[-1] \n",
    "    elif loss_name == CrossEntropyLoss: \n",
    "        D_out = NUM_CATEGORIES\n",
    "\n",
    "    # Create random input and output data\n",
    "\n",
    "    [X_train, Y_train, X_test, Y_test] = \\\n",
    "        [sparse_to_matrix(A) for A in[X_train, Y_train, X_test, Y_test]]\n",
    "\n",
    "    X = torch.tensor(X_train, device=device, dtype=dtype)\n",
    "    if loss_name == CrossEntropyLoss:\n",
    "        y_dtype = torch.int64\n",
    "    else:\n",
    "        y_dtype = dtype\n",
    "    Y = torch.tensor(Y_train, device=device, dtype=y_dtype)\n",
    "#     print (Y)\n",
    "\n",
    "    #create neural network net with multiple hidden layers with H dimetions:\n",
    "    dims = [D_in, *hidden_layers, D_out]\n",
    "    layers = []\n",
    "    for dim_ind in range(len(dims)-2):\n",
    "        layers.append(torch.nn.Linear(dims[dim_ind], dims[dim_ind+1]))\n",
    "        layers.append(torch.nn.ReLU())\n",
    "    layers.append(torch.nn.Dropout(0.5))\n",
    "    layers.append(torch.nn.Linear(dims[-2], D_out))   \n",
    "    \n",
    "    model = torch.nn.Sequential(*layers)\n",
    "    if loss_name == CrossEntropyLoss:\n",
    "        weights = (np.power(1/pd.value_counts(Y.tolist(),normalize=True),1.2)).to_list()\n",
    "        weights = torch.tensor(weights)\n",
    "        loss_fn = torch.nn.CrossEntropyLoss(reduction='mean', weight=weights)     \n",
    "    elif loss_name == MSELoss:\n",
    "        loss_fn = torch.nn.MSELoss(reduction='mean') \n",
    "\n",
    "    # Use the optim package to define an Optimizer that will update the weights of\n",
    "    # the model for us. Here we will use Adam; the optim package contains many other\n",
    "    # optimization algoriths. The first argument to the Adam constructor tells the\n",
    "    # optimizer which Tensors it should update.\n",
    "    learning_rate = 0.005\n",
    "    weight_decay = 0.001\n",
    "    lr_decay = 0.9\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "    scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.9, patience=10000, verbose=True, threshold=0.0001,\n",
    "                                  threshold_mode='rel', cooldown=2000, min_lr=1e-10, eps=1e-08)\n",
    "    \n",
    "    dataloader = torch_data.DataLoader(\n",
    "        torch_data.TensorDataset(X, Y), batch_size=batch_size,\n",
    "        shuffle=True, num_workers=4)\n",
    "    print (\"Start training:\")\n",
    "    print (\"loss name is: \",loss_name, \" Using loss function: \", loss_fn)\n",
    "    accuracy_test(model, X_test, Y_test, data_set_name='test',print_sample = 10, top_n_guess= 1 )\n",
    "    # accuracy_test(model, X_train, Y_train, data_set_name='train',print_sample = 0, top_n_guess= 3)\n",
    "    \n",
    "    epoch_lr = learning_rate\n",
    "    min_loss = 100.0\n",
    "    i_iter =-1\n",
    "    for e in range(epoch):\n",
    "        for t,(x_batch, y_batch) in enumerate(dataloader):\n",
    "            i_iter +=1\n",
    "            # Forward pass: compute predicted y by passing x to the model.\n",
    "            y_pred = model(x_batch)\n",
    "#             print (\"x_batch.shape\", x_batch.shape)\n",
    "#             print (\"y_batch.shape\", y_batch.shape)\n",
    "#             print (\"y_pred.shape\", y_pred.shape)\n",
    "#             print (\"y_batch[0,0]\", y_batch[0,0])\n",
    "#             print (\"y_pred[0,0]\", y_pred[0,0])\n",
    "            y_pred_soft = torch.nn.functional.softmax(y_pred, dim = -1)\n",
    "            # Compute and print loss.\n",
    "#             batch_class_weights = torch.ones((y_batch.dim()))\n",
    "#             loss = loss_fn(y_pred_soft, y_batch, weight = batch_class_weights)\n",
    "#             loss = loss_fn(y_pred_soft, y_batch)\n",
    "#             print (y_batch)\n",
    "            loss = loss_fn(y_pred, y_batch)\n",
    "            if loss < min_loss:\n",
    "                print (f\"loss < min_loss, updating min_loss to {loss}, i_iter {i_iter}\")\n",
    "#                 accuracy_test(model, X_train, Y_train, data_set_name= 'train', print_sample = 0, top_n_guess= 1)\n",
    "#                 accuracy_test(model, X_test, Y_test, data_set_name= 'test', print_sample=0, top_n_guess= 1)\n",
    "                min_loss = loss\n",
    "            if not ( (t +1) % 2000 ) :\n",
    "                print(f\"iter-{t+1}, loss {round(loss.item(),3)}\")\n",
    "\n",
    "            # Before the backward pass, use the optimizer object to zero all of the\n",
    "            # gradients for the variables it will update (which are the learnable\n",
    "            # weights of the model). This is because by default, gradients are\n",
    "            # accumulated in buffers( i.e, not overwritten) whenever .backward()\n",
    "            # is called. Checkout docs of torch.autograd.backward for more details.\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Backward pass: compute gradient of the loss with respect to model\n",
    "            # parameters\n",
    "\n",
    "            #  $$$ this command destroy exit() command $$$\n",
    "            loss.backward()\n",
    "\n",
    "            # Calling the step function on an Optimizer makes an update to its\n",
    "            # parameters\n",
    "            optimizer.step()\n",
    "            \n",
    "            scheduler.step(loss)\n",
    "            \n",
    "            \n",
    "        if not ((e+1) %10): \n",
    "            accuracy_test(model, X_train, Y_train, data_set_name= 'train', print_sample = 0, top_n_guess= 1)\n",
    "            accuracy_test(model, X_test, Y_test, data_set_name= 'test', print_sample=0, top_n_guess= 1)\n",
    "        \n",
    "#         epoch_lr = epoch_lr * lr_decay\n",
    "        print_learning_rate(optimizer)\n",
    "        print(f\"epoch-{e+1}, loss {round(loss.item(),3)}\")\n",
    "        print(\"------------------------------------\")\n",
    "        # set_learning_rate(optimizer,epoch_lr)\n",
    "        \n",
    "    print (\"DONE, returning model\")\n",
    "    return model\n",
    "\n",
    "print (\"updated torch nn\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training:\n",
      "loss name is:  CrossEntropyLoss  Using loss function:  CrossEntropyLoss()\n",
      "Accuracy (top 1 guesses) - test =  7.035 %\n",
      "                                                           0\n",
      "predicted  [[3], [4], [4], [3], [3], [3], [3], [3], [3], ...\n",
      "truth                        [0, 10, 8, 0, 0, 8, 7, 1, 0, 0]\n",
      "\tConfusion Matrix test:\n",
      "        0_T   1_T   2_T   3_T  4_T  5_T  6_T  7_T  8_T  9_T  10_T  11_T  12_T  \\\n",
      "0_P      2     0     0     0    0    0    0    0    0    0     0     0     0   \n",
      "1_P    107    26    22    13   16   11    5    8    4   12     8     2     2   \n",
      "2_P      0     0     0     0    0    0    0    0    0    0     0     0     0   \n",
      "3_P   6932  2128  1426  1268  983  781  729  618  523  479   420   303   301   \n",
      "4_P   2713   837   562   515  399  283  295  220  219  196   185   115   104   \n",
      "5_P      0     0     0     0    0    0    0    0    0    0     0     0     0   \n",
      "6_P      7     4     1     0    0    0    0    0    0    0     0     0     0   \n",
      "7_P      0     0     0     0    0    0    0    0    0    0     0     0     0   \n",
      "8_P      0     0     0     0    0    0    0    0    0    0     0     0     0   \n",
      "9_P      0     0     0     0    0    0    0    0    0    0     0     0     0   \n",
      "10_P     0     0     0     0    0    0    0    0    0    0     0     0     0   \n",
      "11_P     0     0     0     0    0    0    0    0    0    0     0     0     0   \n",
      "12_P     0     0     0     0    0    0    0    0    0    0     0     0     0   \n",
      "13_P     0     0     0     0    0    0    0    0    0    0     0     0     0   \n",
      "\n",
      "      13_T  \n",
      "0_P      0  \n",
      "1_P      2  \n",
      "2_P      0  \n",
      "3_P    220  \n",
      "4_P     89  \n",
      "5_P      0  \n",
      "6_P      0  \n",
      "7_P      0  \n",
      "8_P      0  \n",
      "9_P      0  \n",
      "10_P     0  \n",
      "11_P     0  \n",
      "12_P     0  \n",
      "13_P     0  \n",
      "loss < min_loss, updating min_loss to 2.6623940467834473, i_iter 0\n",
      "loss < min_loss, updating min_loss to 2.6326589584350586, i_iter 2\n",
      "loss < min_loss, updating min_loss to 2.5893332958221436, i_iter 8\n",
      "loss < min_loss, updating min_loss to 2.534088134765625, i_iter 11\n",
      "loss < min_loss, updating min_loss to 2.531125783920288, i_iter 40\n",
      "loss < min_loss, updating min_loss to 2.5174779891967773, i_iter 119\n",
      "loss < min_loss, updating min_loss to 2.459859609603882, i_iter 124\n",
      "loss < min_loss, updating min_loss to 2.397327184677124, i_iter 132\n",
      "loss < min_loss, updating min_loss to 2.3274521827697754, i_iter 145\n",
      "loss < min_loss, updating min_loss to 2.312748670578003, i_iter 165\n",
      "loss < min_loss, updating min_loss to 2.179318428039551, i_iter 176\n",
      "loss < min_loss, updating min_loss to 2.0458984375, i_iter 250\n",
      "loss < min_loss, updating min_loss to 2.0076332092285156, i_iter 265\n",
      "loss < min_loss, updating min_loss to 1.9716132879257202, i_iter 368\n",
      "loss < min_loss, updating min_loss to 1.8138638734817505, i_iter 474\n",
      "loss < min_loss, updating min_loss to 1.7384097576141357, i_iter 512\n",
      "loss < min_loss, updating min_loss to 1.3665920495986938, i_iter 668\n",
      "loss < min_loss, updating min_loss to 1.2853515148162842, i_iter 1673\n",
      "iter-2000, loss 2.404\n",
      "loss < min_loss, updating min_loss to 1.2584202289581299, i_iter 2738\n",
      "loss < min_loss, updating min_loss to 1.1621676683425903, i_iter 2849\n",
      "loss < min_loss, updating min_loss to 1.1016747951507568, i_iter 2850\n",
      "loss < min_loss, updating min_loss to 0.9044862389564514, i_iter 3175\n",
      "lr 0.005\n",
      "epoch-1, loss 2.238\n",
      "------------------------------------\n",
      "loss < min_loss, updating min_loss to 0.8784189820289612, i_iter 3791\n",
      "loss < min_loss, updating min_loss to 0.7792396545410156, i_iter 5286\n",
      "iter-2000, loss 2.055\n",
      "loss < min_loss, updating min_loss to 0.7687622904777527, i_iter 6194\n",
      "loss < min_loss, updating min_loss to 0.6585278511047363, i_iter 6439\n",
      "lr 0.005\n",
      "epoch-2, loss 1.616\n",
      "------------------------------------\n",
      "loss < min_loss, updating min_loss to 0.5375358462333679, i_iter 8533\n",
      "iter-2000, loss 2.028\n",
      "lr 0.005\n",
      "epoch-3, loss 2.169\n",
      "------------------------------------\n",
      "loss < min_loss, updating min_loss to 0.5304885506629944, i_iter 11187\n",
      "iter-2000, loss 1.664\n",
      "loss < min_loss, updating min_loss to 0.4576384127140045, i_iter 12323\n",
      "lr 0.005\n",
      "epoch-4, loss 1.424\n",
      "------------------------------------\n",
      "iter-2000, loss 1.815\n",
      "lr 0.005\n",
      "epoch-5, loss 2.75\n",
      "------------------------------------\n",
      "iter-2000, loss 1.672\n",
      "lr 0.005\n",
      "epoch-6, loss 2.078\n",
      "------------------------------------\n",
      "iter-2000, loss 2.108\n",
      "Epoch 22324: reducing learning rate of group 0 to 4.5000e-03.\n",
      "loss < min_loss, updating min_loss to 0.44216156005859375, i_iter 22663\n",
      "lr 0.0045000000000000005\n",
      "epoch-7, loss 2.732\n",
      "------------------------------------\n",
      "loss < min_loss, updating min_loss to 0.43374207615852356, i_iter 24570\n",
      "iter-2000, loss 1.61\n",
      "lr 0.0045000000000000005\n",
      "epoch-8, loss 2.518\n",
      "------------------------------------\n",
      "iter-2000, loss 1.765\n",
      "lr 0.0045000000000000005\n",
      "epoch-9, loss 2.218\n",
      "------------------------------------\n",
      "loss < min_loss, updating min_loss to 0.381583034992218, i_iter 29891\n",
      "iter-2000, loss 1.488\n",
      "loss < min_loss, updating min_loss to 0.0036762624513357878, i_iter 33079\n",
      "Accuracy (top 1 guesses) - train =  41.374 %\n",
      "\tConfusion Matrix train:\n",
      "        0_T   1_T   2_T   3_T   4_T   5_T  6_T   7_T  8_T  9_T  10_T  11_T  \\\n",
      "0_P   7062     9    43   154    67    38   38    65    5    4     5     0   \n",
      "1_P    387  4445   175   222    82    67   66    33   88   57    31    63   \n",
      "2_P    245    65  1227   171    36    14  219    12   29   61    31   121   \n",
      "3_P    960   107   142  1239    36    22  132    42   23   16    81    23   \n",
      "4_P   1878   192   154   137  1871    78  229    88   66   26    38    49   \n",
      "5_P   2449   121    62   155    57  1709  149    56  933   12    19     3   \n",
      "6_P   1038    74   482   363   100    57  688    28   56   21    40    41   \n",
      "7_P   5516    88   155   250   272    89   78  1341   46   37    64    36   \n",
      "8_P   1219   263   206   199   127   333  219    64  389   73    45    18   \n",
      "9_P    448   231   499   151    83    51  114    35   31  947    60    50   \n",
      "10_P   536   205   272   580   109    40   99    94   28   58   969    50   \n",
      "11_P   341   368   989   263   102    24  177    32   31   39    32   472   \n",
      "12_P   495   117    75   106    32    49   99    22   42   53    20    12   \n",
      "13_P   404   607   202   151   156    18  123    57   66   87    34    43   \n",
      "\n",
      "      12_T  13_T  \n",
      "0_P      4     0  \n",
      "1_P     89    80  \n",
      "2_P     10     8  \n",
      "3_P     22     2  \n",
      "4_P     30    35  \n",
      "5_P     15     4  \n",
      "6_P     19     8  \n",
      "7_P     23    15  \n",
      "8_P     39    30  \n",
      "9_P     93    37  \n",
      "10_P    18    13  \n",
      "11_P    21    31  \n",
      "12_P   456    10  \n",
      "13_P    77   446  \n",
      "Accuracy (top 1 guesses) - test =  38.523 %\n",
      "\tConfusion Matrix test:\n",
      "        0_T   1_T  2_T  3_T  4_T  5_T  6_T  7_T  8_T  9_T  10_T  11_T  12_T  \\\n",
      "0_P   2884     2   19   75   24   19   14   36    2    2     7     2     5   \n",
      "1_P    176  1837   81  102   46   36   45   19   39   24    18    28    46   \n",
      "2_P    108    30  513   83   25    9   84    7   13   35    13    57    14   \n",
      "3_P    371    57   78  480   25   11   60   22   16    7    37     8    18   \n",
      "4_P    784    73   75   72  796   29   86   49   29   28    23    31    19   \n",
      "5_P   1015    67   36   78   30  673   70   27  334   16    17     0     9   \n",
      "6_P    445    39  176  149   43   27  262   13   27   22    15    18    14   \n",
      "7_P   2418    42   75  107  126   45   22  530   33   18    39    12     8   \n",
      "8_P    501   100   98   91   71  149   99   31  143   43    28    10    23   \n",
      "9_P    218   127  203   75   46   21   36   12   20  354    19    36    37   \n",
      "10_P   261    94  122  238   43   14   59   35   17   38   338    36    20   \n",
      "11_P   156   173  390  110   47    8   88   15   11   32    22   148     8   \n",
      "12_P   242    63   33   45   13   21   42   12   24   30    16     6   151   \n",
      "13_P   182   291  112   91   63   13   62   38   38   38    21    28    35   \n",
      "\n",
      "      13_T  \n",
      "0_P      1  \n",
      "1_P     31  \n",
      "2_P      2  \n",
      "3_P      2  \n",
      "4_P     12  \n",
      "5_P      6  \n",
      "6_P      8  \n",
      "7_P     18  \n",
      "8_P     19  \n",
      "9_P     16  \n",
      "10_P     6  \n",
      "11_P     6  \n",
      "12_P    11  \n",
      "13_P   173  \n",
      "lr 0.0045000000000000005\n",
      "epoch-10, loss 0.004\n",
      "------------------------------------\n",
      "iter-2000, loss 0.88\n",
      "lr 0.0045000000000000005\n",
      "epoch-11, loss 1.569\n",
      "------------------------------------\n",
      "iter-2000, loss 1.907\n",
      "lr 0.0045000000000000005\n",
      "epoch-12, loss 2.745\n",
      "------------------------------------\n",
      "iter-2000, loss 2.674\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr 0.0045000000000000005\n",
      "epoch-13, loss 1.151\n",
      "------------------------------------\n",
      "Epoch 43080: reducing learning rate of group 0 to 4.0500e-03.\n",
      "iter-2000, loss 1.804\n",
      "lr 0.004050000000000001\n",
      "epoch-14, loss 0.634\n",
      "------------------------------------\n",
      "iter-2000, loss 1.736\n",
      "lr 0.004050000000000001\n",
      "epoch-15, loss 3.175\n",
      "------------------------------------\n",
      "iter-2000, loss 1.971\n",
      "lr 0.004050000000000001\n",
      "epoch-16, loss 2.712\n",
      "------------------------------------\n",
      "iter-2000, loss 1.905\n",
      "Epoch 55081: reducing learning rate of group 0 to 3.6450e-03.\n",
      "lr 0.0036450000000000007\n",
      "epoch-17, loss 1.002\n",
      "------------------------------------\n",
      "iter-2000, loss 1.417\n",
      "lr 0.0036450000000000007\n",
      "epoch-18, loss 1.555\n",
      "------------------------------------\n",
      "iter-2000, loss 1.834\n",
      "lr 0.0036450000000000007\n",
      "epoch-19, loss 1.86\n",
      "------------------------------------\n",
      "iter-2000, loss 1.035\n",
      "Accuracy (top 1 guesses) - train =  46.426 %\n",
      "\tConfusion Matrix train:\n",
      "        0_T   1_T   2_T   3_T   4_T   5_T   6_T   7_T  8_T  9_T  10_T  11_T  \\\n",
      "0_P   9451     8    46   139    77    68    23   148   10    4     8     3   \n",
      "1_P    456  4494   209   232    99    61    73    33   69   53    28    83   \n",
      "2_P    129    40   886    91    21     6    92     8   12   27    15    90   \n",
      "3_P   2126   187   289  1734    74    54   176    74   51   24   136    42   \n",
      "4_P   1657   102   126    80  1776    51   135   102   60   29    25    19   \n",
      "5_P   1666    97    32    91    45  1622    78    62  886   17    19     2   \n",
      "6_P   1806   208   989   389   219   145  1172    57  126   74    62    73   \n",
      "7_P   2597    22    38    98   159    40    23  1130   14   13    31    18   \n",
      "8_P    902   203   121   151    81   355   119    66  366   32    27    11   \n",
      "9_P    522   188   439   137   110    50    81    66   39  975    62    31   \n",
      "10_P   469   142   187   496   133    54    78   102   37   49   972    33   \n",
      "11_P   340   282  1079   257   131    20   164    39   33   58    36   521   \n",
      "12_P   403   180    50    83    42    31    75    29   30   68    15     9   \n",
      "13_P   454   739   192   163   163    32   141    53  100   68    33    46   \n",
      "\n",
      "      12_T  13_T  \n",
      "0_P      5     1  \n",
      "1_P     72    57  \n",
      "2_P      7     5  \n",
      "3_P     27     5  \n",
      "4_P     15    28  \n",
      "5_P     20     2  \n",
      "6_P     57    23  \n",
      "7_P      8     5  \n",
      "8_P     27    14  \n",
      "9_P     83    30  \n",
      "10_P    18    16  \n",
      "11_P    16    15  \n",
      "12_P   503    19  \n",
      "13_P    58   499  \n",
      "Accuracy (top 1 guesses) - test =  43.084 %\n",
      "\tConfusion Matrix test:\n",
      "        0_T   1_T  2_T  3_T  4_T  5_T  6_T  7_T  8_T  9_T  10_T  11_T  12_T  \\\n",
      "0_P   3904     8   23   45   42   20   14   75    4    3     9     1     6   \n",
      "1_P    188  1871  113  103   48   33   44   20   26   34    23    40    34   \n",
      "2_P     58    22  323   43   16    2   39    2    7   18     6    33     1   \n",
      "3_P    911    91  118  679   49   21   89   49   30   16    46    27    26   \n",
      "4_P    693    48   56   52  730   17   55   55   20   22    14    18    11   \n",
      "5_P    685    57   13   40   18  673   45   31  314   16    20     0    15   \n",
      "6_P    742    82  380  199  103   66  427   33   57   52    32    39    32   \n",
      "7_P   1152    17   24   60   83   20   13  416   16    8    16     2     6   \n",
      "8_P    399    87   48   66   40  122   60   26  143   23    23     7    17   \n",
      "9_P    243   101  190   65   59   24   41   21   31  363    26    24    37   \n",
      "10_P   238    67   98  221   52   24   40   50   16   22   339    19    15   \n",
      "11_P   193   126  481  108   62   16   78   21   21   41    33   173    16   \n",
      "12_P   166    99   32   35   25   14   25   10   19   36     6    10   158   \n",
      "13_P   189   319  112   80   71   23   59   37   42   33    20    27    33   \n",
      "\n",
      "      13_T  \n",
      "0_P      1  \n",
      "1_P     20  \n",
      "2_P      2  \n",
      "3_P      6  \n",
      "4_P     12  \n",
      "5_P      6  \n",
      "6_P     14  \n",
      "7_P      2  \n",
      "8_P     18  \n",
      "9_P     16  \n",
      "10_P     8  \n",
      "11_P     8  \n",
      "12_P    16  \n",
      "13_P   182  \n",
      "lr 0.0036450000000000007\n",
      "epoch-20, loss 1.733\n",
      "------------------------------------\n",
      "Epoch 67082: reducing learning rate of group 0 to 3.2805e-03.\n",
      "iter-2000, loss 1.641\n",
      "lr 0.003280500000000001\n",
      "epoch-21, loss 2.51\n",
      "------------------------------------\n",
      "iter-2000, loss 2.484\n",
      "lr 0.003280500000000001\n",
      "epoch-22, loss 1.05\n",
      "------------------------------------\n",
      "iter-2000, loss 1.246\n",
      "lr 0.003280500000000001\n",
      "epoch-23, loss 1.441\n",
      "------------------------------------\n",
      "iter-2000, loss 1.48\n",
      "Epoch 79083: reducing learning rate of group 0 to 2.9525e-03.\n",
      "lr 0.002952450000000001\n",
      "epoch-24, loss 1.474\n",
      "------------------------------------\n",
      "iter-2000, loss 1.199\n",
      "lr 0.002952450000000001\n",
      "epoch-25, loss 2.004\n",
      "------------------------------------\n",
      "iter-2000, loss 1.746\n",
      "lr 0.002952450000000001\n",
      "epoch-26, loss 0.527\n",
      "------------------------------------\n",
      "iter-2000, loss 1.802\n",
      "lr 0.002952450000000001\n",
      "epoch-27, loss 2.805\n",
      "------------------------------------\n",
      "Epoch 91084: reducing learning rate of group 0 to 2.6572e-03.\n",
      "iter-2000, loss 1.539\n",
      "lr 0.002657205000000001\n",
      "epoch-28, loss 1.394\n",
      "------------------------------------\n",
      "iter-2000, loss 1.101\n",
      "lr 0.002657205000000001\n",
      "epoch-29, loss 1.236\n",
      "------------------------------------\n",
      "iter-2000, loss 1.11\n",
      "Accuracy (top 1 guesses) - train =  47.376 %\n",
      "\tConfusion Matrix train:\n",
      "        0_T   1_T   2_T   3_T   4_T   5_T  6_T   7_T  8_T  9_T  10_T  11_T  \\\n",
      "0_P   9581    12    40   145    62    36   27   117    9    3     3     4   \n",
      "1_P    289  4377   101   116    71    53   40    11   51   33    16    48   \n",
      "2_P    201    24  1003    80    11    10  111     8   15   37    16    63   \n",
      "3_P   1589   144   207  1730    69    34  139    59   26   21   109    18   \n",
      "4_P   2061   103   100   115  1909    47  121   111   41   22    26    15   \n",
      "5_P   1387    32    16    51    24  1255   39    50  447    4     4     1   \n",
      "6_P   1077   117   485   288   150    75  993    36   91   24    37    33   \n",
      "7_P   2065    21    43    61   112    31   14  1164   11    5    19     7   \n",
      "8_P   1477   293   137   218    96   819  208    64  857   33    40    12   \n",
      "9_P    453   179   344   109   100    56   61    53   41  957    44    24   \n",
      "10_P   526   160   159   417   101    42   90   118   37   48  1032    37   \n",
      "11_P   672   474  1711   468   178    28  267    48   53  103    65   657   \n",
      "12_P  1182   465   161   197   110    81  208    65   73  146    32    21   \n",
      "13_P   418   491   176   146   137    22  112    65   81   55    26    41   \n",
      "\n",
      "      12_T  13_T  \n",
      "0_P      5     0  \n",
      "1_P     52    52  \n",
      "2_P      1     3  \n",
      "3_P     13     6  \n",
      "4_P     16    22  \n",
      "5_P      2     0  \n",
      "6_P     21    17  \n",
      "7_P      1     1  \n",
      "8_P     30    19  \n",
      "9_P     63    26  \n",
      "10_P    20     9  \n",
      "11_P    30    29  \n",
      "12_P   625    40  \n",
      "13_P    37   495  \n",
      "Accuracy (top 1 guesses) - test =  42.88 %\n",
      "\tConfusion Matrix test:\n",
      "        0_T   1_T  2_T  3_T  4_T  5_T  6_T  7_T  8_T  9_T  10_T  11_T  12_T  \\\n",
      "0_P   3916     9   24   65   31   13   17   60    7    0     7     1     7   \n",
      "1_P    129  1798   54   55   36   21   27   15   21   11    13    21    34   \n",
      "2_P     89    20  359   37    5    3   50    5    4   23     6    29     1   \n",
      "3_P    653    61  103  647   34   13   69   28   23   14    47    18    15   \n",
      "4_P    877    48   50   64  788   18   58   67   24   26    19    21    12   \n",
      "5_P    605    27   12   31    7  491   21   17  177    1    12     0     2   \n",
      "6_P    437    44  205  134   71   51  345   26   41   38    28    23    17   \n",
      "7_P    925     7   18   34   59   15   10  429    9    8    12     2     2   \n",
      "8_P    650   130   57   90   63  342  113   29  279   34    31    11    20   \n",
      "9_P    201   100  155   51   54   25   38   25   15  344    24    25    28   \n",
      "10_P   256    65   69  208   44   15   40   60   23   35   330    10    18   \n",
      "11_P   309   207  706  209   81   14  106   26   33   52    50   213    21   \n",
      "12_P   512   246   94   89   53   37   72   31   42   73    19    19   215   \n",
      "13_P   202   233  105   82   72   17   63   28   48   28    15    27    15   \n",
      "\n",
      "      13_T  \n",
      "0_P      1  \n",
      "1_P     19  \n",
      "2_P      1  \n",
      "3_P      6  \n",
      "4_P     12  \n",
      "5_P      2  \n",
      "6_P      5  \n",
      "7_P      6  \n",
      "8_P     17  \n",
      "9_P     18  \n",
      "10_P     7  \n",
      "11_P    13  \n",
      "12_P    26  \n",
      "13_P   178  \n",
      "lr 0.002657205000000001\n",
      "epoch-30, loss 1.526\n",
      "------------------------------------\n",
      "iter-2000, loss 1.433\n",
      "lr 0.002657205000000001\n",
      "epoch-31, loss 0.361\n",
      "------------------------------------\n",
      "Epoch 103085: reducing learning rate of group 0 to 2.3915e-03.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter-2000, loss 2.453\n",
      "lr 0.002391484500000001\n",
      "epoch-32, loss 2.456\n",
      "------------------------------------\n",
      "iter-2000, loss 1.56\n",
      "lr 0.002391484500000001\n",
      "epoch-33, loss 1.866\n",
      "------------------------------------\n",
      "iter-2000, loss 1.61\n",
      "lr 0.002391484500000001\n",
      "epoch-34, loss 2.217\n",
      "------------------------------------\n",
      "iter-2000, loss 1.158\n",
      "Epoch 115086: reducing learning rate of group 0 to 2.1523e-03.\n",
      "lr 0.002152336050000001\n",
      "epoch-35, loss 2.391\n",
      "------------------------------------\n",
      "iter-2000, loss 1.54\n",
      "lr 0.002152336050000001\n",
      "epoch-36, loss 2.447\n",
      "------------------------------------\n",
      "iter-2000, loss 1.962\n",
      "lr 0.002152336050000001\n",
      "epoch-37, loss 0.924\n",
      "------------------------------------\n",
      "iter-2000, loss 1.892\n",
      "lr 0.002152336050000001\n",
      "epoch-38, loss 1.915\n",
      "------------------------------------\n",
      "Epoch 127087: reducing learning rate of group 0 to 1.9371e-03.\n",
      "iter-2000, loss 1.547\n",
      "lr 0.001937102445000001\n",
      "epoch-39, loss 0.292\n",
      "------------------------------------\n",
      "iter-2000, loss 1.725\n",
      "Accuracy (top 1 guesses) - train =  52.235 %\n",
      "\tConfusion Matrix train:\n",
      "         0_T   1_T   2_T   3_T   4_T   5_T   6_T   7_T  8_T   9_T  10_T  11_T  \\\n",
      "0_P   11415    26    87   261   107    97    37   132   14     6     8     4   \n",
      "1_P     219  4257   105   111    50    40    35     8   38    17    13    48   \n",
      "2_P     422    77  1692   237    30    18   165    22   18    70    32   135   \n",
      "3_P     946   107   128  1535    48    31    86    25   21     9    86    15   \n",
      "4_P    1965   170   145   167  2028    78   163   114   88    38    40    44   \n",
      "5_P    1007    36    24    51    21  1361    34    25  512     5     4     1   \n",
      "6_P    1424   202   784   448   197   147  1326    66  174    70    81    63   \n",
      "7_P    2772    76    76   116   207   101    56  1311   28    23    33    13   \n",
      "8_P     625   172    42    83    31   508    64    28  681    10    17     2   \n",
      "9_P     370   159   299   107    67    52    51    26   35  1011    42    21   \n",
      "10_P    402   115   124   446    59    46    54   107   40    37  1026    24   \n",
      "11_P    286   369   922   274    86    14   134    20   34    37    35   550   \n",
      "12_P    724   321   102   151    56    64   123    38   59    90    18    20   \n",
      "13_P    401   805   153   154   143    32   102    47   91    68    34    41   \n",
      "\n",
      "      12_T  13_T  \n",
      "0_P     10     1  \n",
      "1_P     26    23  \n",
      "2_P     12     3  \n",
      "3_P     13     1  \n",
      "4_P     24    29  \n",
      "5_P      2     0  \n",
      "6_P     39    36  \n",
      "7_P     17     5  \n",
      "8_P     18     3  \n",
      "9_P     59    12  \n",
      "10_P    15     8  \n",
      "11_P    15    18  \n",
      "12_P   611    17  \n",
      "13_P    55   563  \n",
      "Accuracy (top 1 guesses) - test =  47.25 %\n",
      "\tConfusion Matrix test:\n",
      "        0_T   1_T  2_T  3_T  4_T  5_T  6_T  7_T  8_T  9_T  10_T  11_T  12_T  \\\n",
      "0_P   4724    13   47  109   59   35   23   80   12    5    10     5    15   \n",
      "1_P     94  1741   52   56   22   29   25   11   20    8     8    22    20   \n",
      "2_P    183    44  637   81   15   11   70   12    7   36    22    65     8   \n",
      "3_P    361    47   63  574   25    9   39   22   19    7    32    15    15   \n",
      "4_P    856    80   76   94  816   20   81   68   28   34    29    23    19   \n",
      "5_P    443    29   11   16    8  524   23   17  203    7    14     0     6   \n",
      "6_P    572    79  313  226  116   87  454   37   73   49    38    44    28   \n",
      "7_P   1214    30   43   64  115   39   21  464   34   16    25     7     8   \n",
      "8_P    279    79   19   43   13  222   54    9  218   10    11     4     7   \n",
      "9_P    183    83  139   56   35   24   29   14   22  352    14    21    32   \n",
      "10_P   217    63   66  189   27   14   32   42   23   25   352    12    12   \n",
      "11_P   151   184  375  135   46   11   60   13   22   42    26   159    13   \n",
      "12_P   324   169   68   65   31   30   59   24   23   51    16    14   187   \n",
      "13_P   160   354  102   88   70   20   59   33   42   45    16    29    37   \n",
      "\n",
      "      13_T  \n",
      "0_P      1  \n",
      "1_P     12  \n",
      "2_P      1  \n",
      "3_P      4  \n",
      "4_P     27  \n",
      "5_P      2  \n",
      "6_P     16  \n",
      "7_P     10  \n",
      "8_P     10  \n",
      "9_P     11  \n",
      "10_P     5  \n",
      "11_P     7  \n",
      "12_P    22  \n",
      "13_P   183  \n",
      "lr 0.001937102445000001\n",
      "epoch-40, loss 0.957\n",
      "------------------------------------\n",
      "iter-2000, loss 1.57\n",
      "lr 0.001937102445000001\n",
      "epoch-41, loss 2.972\n",
      "------------------------------------\n",
      "iter-2000, loss 1.581\n",
      "lr 0.001937102445000001\n",
      "epoch-42, loss 4.175\n",
      "------------------------------------\n",
      "Epoch 139088: reducing learning rate of group 0 to 1.7434e-03.\n",
      "iter-2000, loss 0.571\n",
      "lr 0.001743392200500001\n",
      "epoch-43, loss 0.542\n",
      "------------------------------------\n",
      "iter-2000, loss 1.429\n",
      "lr 0.001743392200500001\n",
      "epoch-44, loss 1.602\n",
      "------------------------------------\n",
      "iter-2000, loss 1.858\n",
      "lr 0.001743392200500001\n",
      "epoch-45, loss 1.024\n",
      "------------------------------------\n",
      "iter-2000, loss 2.028\n",
      "Epoch 151089: reducing learning rate of group 0 to 1.5691e-03.\n",
      "lr 0.001569052980450001\n",
      "epoch-46, loss 3.593\n",
      "------------------------------------\n",
      "iter-2000, loss 1.316\n",
      "lr 0.001569052980450001\n",
      "epoch-47, loss 0.743\n",
      "------------------------------------\n",
      "iter-2000, loss 0.913\n",
      "lr 0.001569052980450001\n",
      "epoch-48, loss 2.906\n",
      "------------------------------------\n",
      "iter-2000, loss 1.644\n",
      "lr 0.001569052980450001\n",
      "epoch-49, loss 1.243\n",
      "------------------------------------\n",
      "Epoch 163090: reducing learning rate of group 0 to 1.4121e-03.\n",
      "iter-2000, loss 1.586\n",
      "Accuracy (top 1 guesses) - train =  51.712 %\n",
      "\tConfusion Matrix train:\n",
      "         0_T   1_T   2_T   3_T   4_T   5_T   6_T   7_T  8_T   9_T  10_T  11_T  \\\n",
      "0_P   10319    13    51   169    66    64    19    91    6     2     2     1   \n",
      "1_P     292  4549    92   143    84    42    52    13   41    26    18    39   \n",
      "2_P     722   105  1929   309    45    24   323    47   44    83    39   117   \n",
      "3_P    1196    54   119  1565    32    22    61    24   18     7    66     8   \n",
      "4_P    1864    97    74   107  1973    47   117    80   49    23    31     5   \n",
      "5_P    1249    41    20    78    20  1482    37    23  476     5     8     1   \n",
      "6_P     920   106   369   253   139   100  1090    39  110    19    36    21   \n",
      "7_P    2904    42    77   125   161    81    37  1355   27    14    43     9   \n",
      "8_P     897   189    49   122    73   536    98    46  827    22    12     4   \n",
      "9_P     421   174   280   106    77    63    64    39   36  1034    57     8   \n",
      "10_P    352    97   104   415    59    36    59    70   23    26  1039    17   \n",
      "11_P    835   654  1319   504   243    34   268    65   83    92    72   713   \n",
      "12_P    696   290    76   140    44    42   119    34   33    84    24    11   \n",
      "13_P    311   481   124   105   114    16    86    43   60    54    22    27   \n",
      "\n",
      "      12_T  13_T  \n",
      "0_P      8     1  \n",
      "1_P     40    33  \n",
      "2_P     12     6  \n",
      "3_P      9     0  \n",
      "4_P     17    10  \n",
      "5_P      6     0  \n",
      "6_P     21    15  \n",
      "7_P      6     1  \n",
      "8_P     18     9  \n",
      "9_P     74    24  \n",
      "10_P     7     0  \n",
      "11_P    43    37  \n",
      "12_P   628    13  \n",
      "13_P    27   570  \n",
      "Accuracy (top 1 guesses) - test =  46.242 %\n",
      "\tConfusion Matrix test:\n",
      "        0_T   1_T  2_T  3_T  4_T  5_T  6_T  7_T  8_T  9_T  10_T  11_T  12_T  \\\n",
      "0_P   4224     9   25   83   42   31   16   64    7    3     5     2     4   \n",
      "1_P    121  1856   53   67   45   27   29   13   21   16    18    20    33   \n",
      "2_P    334    62  735  140   29   23  120   25   17   46    21    71    10   \n",
      "3_P    498    30   53  580   19   10   40   18   22    6    32     9    16   \n",
      "4_P    790    50   45   54  802   10   65   57   18   27    20    17    16   \n",
      "5_P    526    29   18   30   12  561   21   21  184   13    11     1     7   \n",
      "6_P    414    36  149  122   78   55  371   18   48   26    18    18    21   \n",
      "7_P   1268    17   25   66   85   28   21  473   23   13    42     2     9   \n",
      "8_P    385    94   36   69   32  234   71   19  272   18    17     6    13   \n",
      "9_P    213    99  133   59   46   33   32   12   23  360    18    18    33   \n",
      "10_P   179    49   53  178   25   11   37   38   16   15   326    10     7   \n",
      "11_P   377   302  557  217  107   28  126   46   42   65    56   212    22   \n",
      "12_P   288   167   58   66   25   14   41   15   22   52    14    14   199   \n",
      "13_P   144   195   71   65   51   10   39   27   31   27    15    20    17   \n",
      "\n",
      "      13_T  \n",
      "0_P      0  \n",
      "1_P     21  \n",
      "2_P      6  \n",
      "3_P      2  \n",
      "4_P     21  \n",
      "5_P      5  \n",
      "6_P      7  \n",
      "7_P      6  \n",
      "8_P     10  \n",
      "9_P     17  \n",
      "10_P     5  \n",
      "11_P    21  \n",
      "12_P    19  \n",
      "13_P   171  \n",
      "lr 0.0014121476824050009\n",
      "epoch-50, loss 1.04\n",
      "------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter-2000, loss 2.16\n",
      "lr 0.0014121476824050009\n",
      "epoch-51, loss 0.816\n",
      "------------------------------------\n",
      "iter-2000, loss 1.055\n",
      "lr 0.0014121476824050009\n",
      "epoch-52, loss 0.315\n",
      "------------------------------------\n",
      "iter-2000, loss 1.544\n",
      "Epoch 175091: reducing learning rate of group 0 to 1.2709e-03.\n",
      "lr 0.0012709329141645008\n",
      "epoch-53, loss 0.325\n",
      "------------------------------------\n",
      "iter-2000, loss 1.365\n",
      "lr 0.0012709329141645008\n",
      "epoch-54, loss 1.284\n",
      "------------------------------------\n",
      "iter-2000, loss 0.995\n",
      "lr 0.0012709329141645008\n",
      "epoch-55, loss 1.323\n",
      "------------------------------------\n",
      "iter-2000, loss 0.967\n",
      "lr 0.0012709329141645008\n",
      "epoch-56, loss 2.911\n",
      "------------------------------------\n",
      "Epoch 187092: reducing learning rate of group 0 to 1.1438e-03.\n",
      "iter-2000, loss 1.062\n",
      "lr 0.0011438396227480508\n",
      "epoch-57, loss 1.025\n",
      "------------------------------------\n",
      "iter-2000, loss 1.914\n",
      "lr 0.0011438396227480508\n",
      "epoch-58, loss 3.303\n",
      "------------------------------------\n",
      "iter-2000, loss 1.121\n",
      "lr 0.0011438396227480508\n",
      "epoch-59, loss 3.473\n",
      "------------------------------------\n",
      "iter-2000, loss 1.594\n",
      "Accuracy (top 1 guesses) - train =  51.95 %\n",
      "\tConfusion Matrix train:\n",
      "         0_T   1_T   2_T   3_T   4_T   5_T   6_T   7_T   8_T   9_T  10_T  11_T  \\\n",
      "0_P   10314     9    59   141    55    36    12    72     6     2     2     4   \n",
      "1_P     185  4256    75    72    40    32    23     7    17    12     6    31   \n",
      "2_P     523    90  1817   235    32    17   211    23    15    51    24    88   \n",
      "3_P    1393    92   167  1781    56    17    79    21    28     7    58    11   \n",
      "4_P    1802   103    86    98  2015    47   110    78    48    13    19    14   \n",
      "5_P     995    14    27    36    12  1025    24    17   130     5     2     0   \n",
      "6_P    1102   152   492   296   150    92  1224    45    99    27    44    24   \n",
      "7_P    2666    45    61   100   154    57    26  1376    19    15    25     9   \n",
      "8_P    1561   341    99   188    91  1098   161    61  1239    46    23     8   \n",
      "9_P     426   204   309   111    66    46    69    37    42  1071    50    17   \n",
      "10_P    504   179   141   535   116    40   102   108    27    37  1132    23   \n",
      "11_P    479   495  1103   308   150    13   175    40    52    63    49   709   \n",
      "12_P    637   360    94   125    51    48    93    41    40    90    15    14   \n",
      "13_P    391   552   153   115   142    21   121    43    71    52    20    29   \n",
      "\n",
      "      12_T  13_T  \n",
      "0_P      6     1  \n",
      "1_P     25    14  \n",
      "2_P      6     4  \n",
      "3_P     16     3  \n",
      "4_P     11    10  \n",
      "5_P      2     0  \n",
      "6_P     16     2  \n",
      "7_P      6     4  \n",
      "8_P     22    14  \n",
      "9_P     98    24  \n",
      "10_P    13     8  \n",
      "11_P    17    24  \n",
      "12_P   655    18  \n",
      "13_P    23   593  \n",
      "Accuracy (top 1 guesses) - test =  45.3 %\n",
      "\tConfusion Matrix test:\n",
      "        0_T   1_T  2_T  3_T  4_T  5_T  6_T  7_T  8_T  9_T  10_T  11_T  12_T  \\\n",
      "0_P   4188    10   28   73   37   15   16   55   13    2     6     2     7   \n",
      "1_P     66  1753   41   45   22   23   26    9   12    8     7    17    29   \n",
      "2_P    221    47  680   87   30   13   87    9    7   39    22    72    12   \n",
      "3_P    580    49   80  655   28    9   49   25   24   11    29     9    15   \n",
      "4_P    759    54   51   53  802   21   63   58   22   25    20    18    15   \n",
      "5_P    406    11   13   15    9  360   17   11   77    6    11     0     4   \n",
      "6_P    493    61  227  143   83   59  376   22   50   38    27    17    19   \n",
      "7_P   1211    20   22   58   85   25   21  470   18   14    22     5    11   \n",
      "8_P    691   165   50   91   49  463   94   32  385   33    27     8    23   \n",
      "9_P    212   110  148   66   41   22   34   14   23  345    21    22    42   \n",
      "10_P   253    93   63  225   46   17   42   72   21   21   360    18    14   \n",
      "11_P   221   213  459  149   70   15   98   25   32   45    31   191    15   \n",
      "12_P   287   181   63   62   29   17   43   20   27   70    16    15   177   \n",
      "13_P   173   228   86   74   67   16   63   24   35   30    14    26    24   \n",
      "\n",
      "      13_T  \n",
      "0_P      1  \n",
      "1_P     17  \n",
      "2_P      4  \n",
      "3_P      5  \n",
      "4_P     19  \n",
      "5_P      2  \n",
      "6_P      5  \n",
      "7_P      7  \n",
      "8_P     23  \n",
      "9_P     18  \n",
      "10_P     8  \n",
      "11_P     9  \n",
      "12_P    20  \n",
      "13_P   173  \n",
      "lr 0.0011438396227480508\n",
      "epoch-60, loss 0.139\n",
      "------------------------------------\n",
      "Epoch 199093: reducing learning rate of group 0 to 1.0295e-03.\n",
      "iter-2000, loss 0.788\n",
      "lr 0.0010294556604732458\n",
      "epoch-61, loss 0.922\n",
      "------------------------------------\n",
      "iter-2000, loss 1.325\n",
      "lr 0.0010294556604732458\n",
      "epoch-62, loss 0.18\n",
      "------------------------------------\n",
      "iter-2000, loss 0.862\n",
      "lr 0.0010294556604732458\n",
      "epoch-63, loss 0.31\n",
      "------------------------------------\n",
      "iter-2000, loss 1.711\n",
      "Epoch 211094: reducing learning rate of group 0 to 9.2651e-04.\n",
      "lr 0.0009265100944259213\n",
      "epoch-64, loss 1.517\n",
      "------------------------------------\n",
      "iter-2000, loss 0.765\n",
      "lr 0.0009265100944259213\n",
      "epoch-65, loss 3.081\n",
      "------------------------------------\n",
      "iter-2000, loss 2.589\n",
      "lr 0.0009265100944259213\n",
      "epoch-66, loss 0.96\n",
      "------------------------------------\n",
      "iter-2000, loss 1.276\n",
      "lr 0.0009265100944259213\n",
      "epoch-67, loss 0.908\n",
      "------------------------------------\n",
      "Epoch 223095: reducing learning rate of group 0 to 8.3386e-04.\n",
      "iter-2000, loss 1.989\n"
     ]
    }
   ],
   "source": [
    "# train model using pytorch\n",
    "# model = torch_net(X_train, Y_train[:,0],X_test,Y_test,[100,50,50],epoch=1000)\n",
    "model = torch_net(X_train, Y_train[:,0],X_test,Y_test,[100,70,50],epoch=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
