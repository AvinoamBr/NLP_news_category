{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['POLITICS', 'FOOD & DRINK', 'WEDDINGS', 'WORLDPOST', 'ARTS', 'EDUCATION']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import nltk\n",
    "import sklearn\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import re\n",
    "import torch.utils.data as torch_data\n",
    "import scipy\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "# from Utils.pytorch_utils import torch_net\n",
    "from Utils.pytorch_utils import sparse_to_matrix #, accuracy_test\n",
    "\n",
    "from Utils.NLP_utils import accuracy, find_senteces_with_lemma, get_wordnet_pos, load_and_lemmatize_data, load_processed_data\n",
    "\n",
    "# pickle file, data set as readable json file, since original data set is a 'pseudo json', written in text file.\n",
    "DATA_SET_FILE = r\"datasets\\News_Category_Dataset_v2_mod.pkl\"\n",
    "PROCESSED_DATA_SET = r\"datasets\\News_Category_Dataset_v2_mod_processed.pkl\"\n",
    "\n",
    "ALL_CATEGORIES = ['POLITICS', 'WELLNESS', 'ENTERTAINMENT', 'TRAVEL', 'STYLE & BEAUTY',\n",
    "       'PARENTING', 'HEALTHY LIVING', 'QUEER VOICES', 'FOOD & DRINK',\n",
    "       'BUSINESS', 'COMEDY', 'SPORTS', 'BLACK VOICES', 'HOME & LIVING',\n",
    "       'PARENTS', 'THE WORLDPOST', 'WEDDINGS', 'WOMEN', 'IMPACT', 'DIVORCE',\n",
    "       'CRIME', 'MEDIA', 'WEIRD NEWS', 'GREEN', 'WORLDPOST', 'RELIGION',\n",
    "       'STYLE', 'SCIENCE', 'WORLD NEWS', 'TASTE', 'TECH', 'MONEY', 'ARTS',\n",
    "       'FIFTY', 'GOOD NEWS', 'ARTS & CULTURE', 'ENVIRONMENT', 'COLLEGE',\n",
    "       'LATINO VOICES', 'CULTURE & ARTS', 'EDUCATION']\n",
    "\n",
    "# REQUIRED_CATEGORIES = ['RELIGION','SCIENCE', 'TASTE','PARENTING' , 'COLLEGE' ,'POLITICS' ]\n",
    "REQUIRED_CATEGORIES = (np.array(ALL_CATEGORIES)[:10]).tolist()\n",
    "print (REQUIRED_CATEGORIES)\n",
    "NUM_CATEGORIES = len(REQUIRED_CATEGORIES)\n",
    "    \n",
    "    \n",
    "CrossEntropyLoss = \"CrossEntropyLoss\"\n",
    "MSELoss = \"MSELoss\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# set loss type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_name = CrossEntropyLoss\n",
    "# loss_name = MSELoss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sparse_to_matrix(A):\n",
    "    if type(A) in [scipy.sparse.csr.csr_matrix, torch.sparse]:\n",
    "        print (\"type of matrix onverted\")\n",
    "        return np.array(A.todense())\n",
    "    return A\n",
    "\n",
    "def spy_sparse2torch_sparse(data):\n",
    "    \"\"\"\n",
    "\n",
    "    :param data: a scipy sparse csr matrix\n",
    "    :return: a sparse torch tensor\n",
    "    \"\"\"\n",
    "    samples=data.shape[0]\n",
    "    features=data.shape[1]\n",
    "    values=data.data\n",
    "    coo_data=data.tocoo()\n",
    "    indices=torch.LongTensor([coo_data.row,coo_data.col])\n",
    "    t=torch.sparse.FloatTensor(indices,torch.from_numpy(values).float(),[samples,features])\n",
    "    return t\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def confusion_matrix_disp(y_pred,y_true,label = False):\n",
    "    cm = confusion_matrix(y_pred,y_true)\n",
    "    if not label:\n",
    "        label = range(NUM_CATEGORIES)\n",
    "    cm_pd = pd.DataFrame(cm,index = [\"{}_P\".format(i)  for i in label],columns = [\"{}_T\".format(i)  for i in label])\n",
    "    return cm_pd\n",
    "\n",
    "def accuracy_test(model, x, y, data_set_name = 'test',print_sample = False,top_n_guess = 1):\n",
    "          \n",
    "    if loss_name == MSELoss:\n",
    "        truth = np.argmax(y, axis=-1)\n",
    "    elif loss_name == CrossEntropyLoss:\n",
    "        truth = y.ravel()\n",
    "    \n",
    "    model_result = model(torch.tensor(x, dtype=torch.float))\n",
    "    top_guesses = torch.argsort( model_result , dim = -1, descending=True ).numpy() [:,:top_n_guess]\n",
    "    # predicted   = torch.argmax ( model_result, dim=-1).numpy()\n",
    "    true_predicted = (top_guesses == truth[:,None]).any(1)\n",
    "    \n",
    "\n",
    "    print (f\"Accuracy (top {top_n_guess} guesses) - {data_set_name} = \",\n",
    "           round( np.array(true_predicted).mean()* 100, 3 ),\n",
    "           \"%\")\n",
    "    if print_sample:\n",
    "        ps = print_sample\n",
    "        sample = pd.DataFrame((top_guesses[:ps],truth[:ps]),index=['predicted','truth'])\n",
    "        print (sample)\n",
    "        # print (pd.value_counts(predicted))\n",
    "    print (f\"\\tConfusion Matrix {data_set_name}:\\n\",confusion_matrix_disp (top_guesses[:,0],truth))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading data\n",
    "\n",
    "# dataset, headlines, headlines_orig = load_and_lemmatize_data(DATA_SET_FILE)\n",
    "dataset, headlines, headlines_orig = load_processed_data(PROCESSED_DATA_SET)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# reduce dataset to n categories:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = dataset['category']\n",
    "pd.value_counts(categories)\n",
    "\n",
    "# filter data for desired categories, to make problem easier\n",
    "filter_categories = True\n",
    "if filter_categories:\n",
    "    filter_index =  categories.isin(REQUIRED_CATEGORIES)\n",
    "    dataset   = dataset[filter_index]\n",
    "    headlines = np.array(headlines)[filter_index]\n",
    "    headlines_orig = np.array(headlines_orig)[filter_index]\n",
    "    \n",
    "else:\n",
    "    NUM_CATEGORIES = len(set(categories))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def categories_to_index(categories):\n",
    "    d = {}\n",
    "    for i, cat in enumerate(REQUIRED_CATEGORIES):\n",
    "        d[cat] = i\n",
    "        \n",
    "    r = np.array(range(len(categories)))\n",
    "\n",
    "    for cat,i in d.items():\n",
    "        # print (cat,i)\n",
    "        r[categories == cat ] = i\n",
    "    return r\n",
    "\n",
    "categories = dataset['category']\n",
    "if loss_name == CrossEntropyLoss:  \n",
    "    Y = categories_to_index(categories)[:,np.newaxis]\n",
    "else:\n",
    "    Y  = np.array(pd.get_dummies(categories)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data and lables to train/test\n",
    "\n",
    "headlines_train, headlines_test,\\\n",
    "headlines_train_orig, headlines_test_orig,\\\n",
    "Y_train, Y_test,\\\n",
    "    = sklearn.model_selection.train_test_split(\n",
    "    headlines,headlines_orig, Y, test_size = 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract features (Bag Of Words) using Vectorizer\n",
    "\n",
    "max_features=2000\n",
    "\n",
    "vectorizer = CountVectorizer\n",
    "# vectorizer = TfidfVectorizer\n",
    "matrix = vectorizer(max_features=max_features, ngram_range=(1, 2), max_df=0.1 ,min_df = 5)\n",
    "matrix.fit(headlines_train)\n",
    "X_train = matrix.transform(headlines_train)# .todense()\n",
    "X_test = matrix.transform(headlines_test)# .todense()\n",
    "\n",
    "# --- convert to data frame for display and debug ---\n",
    "# tokens = matrix.get_feature_names()\n",
    "# X_train= pd.DataFrame(X_train,columns=tokens)\n",
    "# X_test= pd.DataFrame(X_test,columns=tokens)\n",
    "\n",
    "assert X_train.shape[1]==max_features, X_train.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, X_test) = [spy_sparse2torch_sparse(A) for A in (X_train, X_test)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_learning_rate(optimizer):\n",
    "        for param_group in optimizer.param_groups:\n",
    "            print('lr', param_group['lr'])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "updated torch nn\n"
     ]
    }
   ],
   "source": [
    "def torch_net(X_train, Y_train, X_test, Y_test,\n",
    "              hidden_layers=[10], device=torch.device('cpu'), n_epoch=30, batch_size=17):\n",
    "    \n",
    "    dtype = torch.float\n",
    "    # device = torch.device(\"cuda:0\") # Uncomment this to run on GPU\n",
    "\n",
    "    # N is batch size; D_in is input dimension;\n",
    "    # H is hidden dimension; D_out is output dimension.\n",
    "    N, D_in = X_train.shape\n",
    "    \n",
    "    if loss_name == MSELoss: \n",
    "        D_out = Y_train.shape[-1] \n",
    "    elif loss_name == CrossEntropyLoss: \n",
    "        D_out = NUM_CATEGORIES\n",
    "\n",
    "    # Create random input and output data\n",
    "    [X_train, Y_train, X_test, Y_test] = \\\n",
    "        [sparse_to_matrix(A) for A in[X_train, Y_train, X_test, Y_test]]\n",
    "\n",
    "    X = torch.tensor(X_train, device=device, dtype=dtype)\n",
    "    if loss_name == CrossEntropyLoss:\n",
    "        y_dtype = torch.int64\n",
    "    else:\n",
    "        y_dtype = dtype\n",
    "    Y = torch.tensor(Y_train, device=device, dtype=y_dtype)\n",
    "#     print (Y)\n",
    "\n",
    "    #create neural network net with multiple hidden layers with H dimetions:\n",
    "    dims = [D_in, *hidden_layers, D_out]\n",
    "    layers = []\n",
    "    for dim_ind in range(len(dims)-2):\n",
    "        layers.append(torch.nn.Linear(dims[dim_ind], dims[dim_ind+1]))\n",
    "        layers.append(torch.nn.ReLU())\n",
    "    layers.append(torch.nn.Dropout(0.5))\n",
    "    layers.append(torch.nn.Linear(dims[-2], D_out))   \n",
    "    \n",
    "    model = torch.nn.Sequential(*layers)\n",
    "    if loss_name == CrossEntropyLoss:\n",
    "        weights = (np.power(1/pd.value_counts(Y.tolist(),normalize=True),1.2)).to_list()\n",
    "        weights = torch.tensor(weights)\n",
    "        loss_fn = torch.nn.CrossEntropyLoss(reduction='mean', weight=weights)     \n",
    "    elif loss_name == MSELoss:\n",
    "        loss_fn = torch.nn.MSELoss(reduction='mean') \n",
    "\n",
    "    # Use the optim package to define an Optimizer that will update the weights of\n",
    "    # the model for us. Here we will use Adam; the optim package contains many other\n",
    "    # optimization algoriths. The first argument to the Adam constructor tells the\n",
    "    # optimizer which Tensors it should update.\n",
    "    learning_rate = 0.005\n",
    "    weight_decay = 0.001\n",
    "    lr_decay = 0.9\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "    scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.9, patience=10000, verbose=True, threshold=0.0001,\n",
    "                                  threshold_mode='rel', cooldown=2000, min_lr=1e-10, eps=1e-08)\n",
    "  \n",
    "    dataloader = torch_data.DataLoader(\n",
    "        torch_data.TensorDataset(X.to_dense(), Y), \n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        # num_workers=4,\n",
    "        )\n",
    "#     print (\"Start training:\")\n",
    "#     print (\"loss name is: \",loss_name, \" Using loss function: \", loss_fn)\n",
    "#     accuracy_test(model, X_test, Y_test, data_set_name='test',print_sample = 10, top_n_guess= 1 )\n",
    "    # accuracy_test(model, X_train, Y_train, data_set_name='train',print_sample = 0, top_n_guess= 3)\n",
    "    \n",
    "    epoch_lr = learning_rate\n",
    "    \n",
    "    \n",
    "    return model, dataloader, optimizer, loss_fn, scheduler\n",
    "    \n",
    "print (\"updated torch nn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, n_epoch, dataloader, optimizer, loss_fn, scheduler):\n",
    "    enumerate(dataloader)\n",
    "    i_iter =-1\n",
    "    min_loss = 100.0\n",
    "\n",
    "    for e in range(n_epoch):\n",
    "        for t,(x_batch, y_batch) in enumerate(dataloader):\n",
    "            i_iter +=1\n",
    "            # Forward pass: compute predicted y by passing x to the model.\n",
    "            y_pred = model(x_batch)\n",
    "\n",
    "            # y_pred_soft  is need for MSE computation\n",
    "            # y_pred_soft = torch.nn.functional.softmax(y_pred, dim = -1)\n",
    "            # Compute and print loss.\n",
    "            #  batch_class_weights = torch.ones((y_batch.dim()))\n",
    "            # loss = loss_fn(y_pred_soft, y_batch, weight = batch_class_weights)\n",
    "            # loss = loss_fn(y_pred_soft, y_batch)\n",
    "\n",
    "            loss = loss_fn(y_pred, y_batch)\n",
    "            if loss < min_loss:\n",
    "                print (f\"loss < min_loss, updating min_loss to {loss}, i_iter {i_iter}\")\n",
    "#                 accuracy_test(model, X_train, Y_train, data_set_name= 'train', print_sample = 0, top_n_guess= 1)\n",
    "#                 accuracy_test(model, X_test, Y_test, data_set_name= 'test', print_sample=0, top_n_guess= 1)\n",
    "                min_loss = loss\n",
    "            if not ( (t +1) % 2000 ) :\n",
    "                print(f\"iter-{t+1}, loss {round(loss.item(),3)}\")\n",
    "\n",
    "            # Before the backward pass, use the optimizer object to zero all of the\n",
    "            # gradients for the variables it will update (which are the learnable\n",
    "            # weights of the model). This is because by default, gradients are\n",
    "            # accumulated in buffers( i.e, not overwritten) whenever .backward()\n",
    "            # is called. Checkout docs of torch.autograd.backward for more details.\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Backward pass: compute gradient of the loss with respect to model\n",
    "            # parameters\n",
    "\n",
    "            #  $$$ this command destroy exit() command $$$\n",
    "            loss.backward()\n",
    "\n",
    "            # Calling the step function on an Optimizer makes an update to its\n",
    "            # parameters\n",
    "            optimizer.step()\n",
    "\n",
    "            if scheduler:\n",
    "                scheduler.step(loss)\n",
    "\n",
    "\n",
    "        if not ((e+1) %3): \n",
    "            accuracy_test(model, X_train, Y_train, data_set_name= 'train', print_sample = 0, top_n_guess= 1)\n",
    "            accuracy_test(model, X_test, Y_test, data_set_name= 'test', print_sample=0, top_n_guess= 1)\n",
    "\n",
    "#         epoch_lr = epoch_lr * lr_decay\n",
    "        print_learning_rate(optimizer)\n",
    "        print(f\"epoch-{e+1}, loss {round(loss.item(),3)}\")\n",
    "        print(\"------------------------------------\")\n",
    "        # set_learning_rate(optimizer,epoch_lr)\n",
    "\n",
    "    print (\"DONE, returning model\")\n",
    "    return model, optimizer, loss, n_epoch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define model, optimizer, loss, dataloader, scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:20: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    }
   ],
   "source": [
    "# train model using pytorch\n",
    "# model = torch_net(X_train, Y_train[:,0],X_test,Y_test,[100,50,50],epoch=1000)\n",
    "model, dataloader, optimizer, loss_fn, scheduler =  torch_net(X_train, Y_train[:,0],X_test,Y_test,[100,70,50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before training:\n",
      "loss name is:  CrossEntropyLoss  Using loss function:  CrossEntropyLoss()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:15: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy (top 1 guesses) - test =  38.85 %\n",
      "                                                           0\n",
      "predicted  [[13], [8], [0], [13], [3], [11], [9], [7], [9...\n",
      "truth                         [2, 0, 0, 5, 3, 4, 2, 0, 9, 6]\n",
      "\tConfusion Matrix test:\n",
      "        0_T   1_T   2_T  3_T  4_T  5_T  6_T  7_T  8_T  9_T  10_T  11_T  12_T  \\\n",
      "0_P   3688     7    54   93   24   65   20   82   25    6    13     5    10   \n",
      "1_P    185  1797   125   80   52   33   51   18   54   41    19    22    48   \n",
      "2_P     55     5    27   19    4    3   26    2    2    3     2     4     1   \n",
      "3_P    765    80    94  572   33   30   85   45   32   17    77    19    16   \n",
      "4_P    530    57    50   39  648   27   61   47   26   22    14    22    15   \n",
      "5_P    575    18    16   29   16  537   25   21  276    9     8     1     7   \n",
      "6_P    487    55   161  109   81   61  310   18   51   15    34    16    10   \n",
      "7_P   1976    30    35  117  154   50   32  454   28   20    22     7    11   \n",
      "8_P    336    62    40   37   32  194   34   18  125   15    13     6    15   \n",
      "9_P    167    55   114   26   49   17   29   21   12  267    16    19    11   \n",
      "10_P   186    37    46  106   32   17   37   34   16   19   331    16     8   \n",
      "11_P   500   419  1147  343   94   33  235   42   66  111    51   273    33   \n",
      "12_P   349   136    70   76   44   42   56   21   40   89    15    11   183   \n",
      "13_P    90   235    49   35   49   12   30   10   25   13    13    15    28   \n",
      "\n",
      "      13_T  \n",
      "0_P      2  \n",
      "1_P     31  \n",
      "2_P      1  \n",
      "3_P      7  \n",
      "4_P     18  \n",
      "5_P      2  \n",
      "6_P     13  \n",
      "7_P      9  \n",
      "8_P      8  \n",
      "9_P     11  \n",
      "10_P     3  \n",
      "11_P    36  \n",
      "12_P    32  \n",
      "13_P   149  \n",
      "Accuracy (top 1 guesses) - train =  41.799 %\n",
      "\tConfusion Matrix train:\n",
      "        0_T   1_T   2_T   3_T   4_T   5_T  6_T   7_T  8_T  9_T  10_T  11_T  \\\n",
      "0_P   8868    22    88   202    75   138   48   163   69   17    15     5   \n",
      "1_P    498  4260   193   207    82    97   90    43   98   66    45    53   \n",
      "2_P    128     9    68    33    10     7   66     3    4   10     4     4   \n",
      "3_P   1751   177   223  1541    77    55  160    93   58   21   183    24   \n",
      "4_P   1244   133   130    94  1679    53  132   117   60   55    37    23   \n",
      "5_P   1219    64    26    61    30  1341   60    38  683   19    14     1   \n",
      "6_P   1087   130   312   285   205   110  911    40  102   29    42    27   \n",
      "7_P   4380    70    94   257   383    91   53  1225   56   18    49     7   \n",
      "8_P    694   157    76   117    44   404   82    30  363   41    22    13   \n",
      "9_P    408   129   275    82    84    40   67    32   31  778    42    33   \n",
      "10_P   445    90   103   306    87    29   54    58   35   44   803    19   \n",
      "11_P  1169   874  2870   825   257    63  542    78  113  199   133   716   \n",
      "12_P   772   290   113   173    71    94  121    41   96  183    48    21   \n",
      "13_P   187   489    95    73   132    21   42    21   33   51    17    19   \n",
      "\n",
      "      12_T  13_T  \n",
      "0_P     17     8  \n",
      "1_P     72    58  \n",
      "2_P      1     2  \n",
      "3_P     23    10  \n",
      "4_P     14    44  \n",
      "5_P      8     1  \n",
      "6_P     38    11  \n",
      "7_P     19    15  \n",
      "8_P     23     5  \n",
      "9_P     37    23  \n",
      "10_P    17     9  \n",
      "11_P    74    50  \n",
      "12_P   537    62  \n",
      "13_P    47   410  \n",
      "loss < min_loss, updating min_loss to 1.9849504232406616, i_iter 0\n",
      "loss < min_loss, updating min_loss to 1.5177289247512817, i_iter 1\n",
      "loss < min_loss, updating min_loss to 1.461600661277771, i_iter 4\n",
      "loss < min_loss, updating min_loss to 1.0888620615005493, i_iter 5\n",
      "loss < min_loss, updating min_loss to 0.5419079065322876, i_iter 29\n",
      "iter-2000, loss 1.538\n",
      "lr 0.005\n",
      "epoch-1, loss 1.947\n",
      "------------------------------------\n",
      "Epoch 19689: reducing learning rate of group 0 to 4.5000e-03.\n",
      "loss < min_loss, updating min_loss to 0.4391554296016693, i_iter 3944\n",
      "iter-2000, loss 2.627\n",
      "lr 0.0045000000000000005\n",
      "epoch-2, loss 1.098\n",
      "------------------------------------\n",
      "iter-2000, loss 0.942\n",
      "Accuracy (top 1 guesses) - train =  35.344 %\n",
      "\tConfusion Matrix train:\n",
      "        0_T   1_T   2_T   3_T   4_T  5_T  6_T   7_T  8_T  9_T  10_T  11_T  \\\n",
      "0_P   4082     1    24    37     7   15    8    23    1    1     3     1   \n",
      "1_P    219  3851   123    96    47   44   45     7   35   29    13    52   \n",
      "2_P    383   174  1214   272    31   16  151     8   19   43    49   240   \n",
      "3_P   1826   278   276  1726    42   36  185    67   52   26   139    32   \n",
      "4_P   2390   236   279   191  2059  111  351   218  120   80    41    53   \n",
      "5_P    755    46     8    46     4  906   25    19  416    3     3     0   \n",
      "6_P   1185   171   445   248   149  139  896    34  128   25    36    52   \n",
      "7_P   6581    43    66   203   206   68   33  1196   26   15    33    10   \n",
      "8_P   1562   220    73   116    60  940   86    78  680   28    21    10   \n",
      "9_P    881   277   496   164   120   71  144    56   74  953    80    38   \n",
      "10_P   745   184   142   512   121   57   86   143   48   45   894    23   \n",
      "11_P   634   436  1292   368   174   39  236    55   90  130    87   416   \n",
      "12_P  1298   274   123   175    60   80  127    38   59  110    33    14   \n",
      "13_P   309   703   105   102   136   21   55    40   53   43    22    24   \n",
      "\n",
      "      12_T  13_T  \n",
      "0_P      5     0  \n",
      "1_P     37    25  \n",
      "2_P     13     2  \n",
      "3_P     31     8  \n",
      "4_P     23    53  \n",
      "5_P      1     1  \n",
      "6_P     47    11  \n",
      "7_P     10     4  \n",
      "8_P     23    16  \n",
      "9_P     81    39  \n",
      "10_P    15    20  \n",
      "11_P    31    41  \n",
      "12_P   548    38  \n",
      "13_P    62   450  \n",
      "Accuracy (top 1 guesses) - test =  32.546 %\n",
      "\tConfusion Matrix test:\n",
      "        0_T   1_T  2_T  3_T  4_T  5_T  6_T  7_T  8_T  9_T  10_T  11_T  12_T  \\\n",
      "0_P   1708     0   14   17    4    6    4   13    2    1     0     0     1   \n",
      "1_P     94  1623   70   39   30   20   16    6   19   16     6    23    24   \n",
      "2_P    151    93  519  121   13    7   54   10    7   23    16    77     9   \n",
      "3_P    786   115  121  602   20   29   95   30   37   17    55    15    15   \n",
      "4_P   1035   129  120   81  780   64  151  104   56   45    42    41    22   \n",
      "5_P    351    16    6   18    8  369   10   11  171    2     5     0     3   \n",
      "6_P    555    71  196  119   66   66  337   21   54   32    24    34    16   \n",
      "7_P   2808    20   32   82   87   28   19  435   11    2    19    11    10   \n",
      "8_P    662    92   29   48   31  399   37   39  263   13    12     5    14   \n",
      "9_P    395   111  210   77   63   31   66   30   38  345    21    27    46   \n",
      "10_P   308    96   65  187   59   36   53   67   21   24   366    21     9   \n",
      "11_P   284   184  528  169   67   19   97   29   39   51    31   154    18   \n",
      "12_P   612   132   62   85   26   29   60   17   29   58    22    11   187   \n",
      "13_P   140   311   56   36   58   18   32   21   31   18     9    17    22   \n",
      "\n",
      "      13_T  \n",
      "0_P      0  \n",
      "1_P     15  \n",
      "2_P      4  \n",
      "3_P     11  \n",
      "4_P     25  \n",
      "5_P      1  \n",
      "6_P     15  \n",
      "7_P      1  \n",
      "8_P     10  \n",
      "9_P     22  \n",
      "10_P     6  \n",
      "11_P    35  \n",
      "12_P    23  \n",
      "13_P   154  \n",
      "lr 0.0045000000000000005\n",
      "epoch-3, loss 1.112\n",
      "------------------------------------\n",
      "DONE, returning model\n",
      "After training:\n",
      "loss name is:  CrossEntropyLoss  Using loss function:  CrossEntropyLoss()\n",
      "Accuracy (top 1 guesses) - test =  32.613 %\n",
      "                                                           0\n",
      "predicted  [[6], [3], [7], [13], [3], [11], [11], [7], [9...\n",
      "truth                         [2, 0, 0, 5, 3, 4, 2, 0, 9, 6]\n",
      "\tConfusion Matrix test:\n",
      "        0_T   1_T  2_T  3_T  4_T  5_T  6_T  7_T  8_T  9_T  10_T  11_T  12_T  \\\n",
      "0_P   1709     0   15   20    2    7    6   13    0    2     1     0     1   \n",
      "1_P     84  1616   61   42   25   14   16    7   25   17    10    27    27   \n",
      "2_P    145    86  497  116   12   11   64    9   10   26    13    84    10   \n",
      "3_P    783   125  123  630   21   25   88   30   31   14    72    15    13   \n",
      "4_P   1039   116  116   76  781   65  145  103   61   53    38    35    22   \n",
      "5_P    335    26    7   17    8  398   10    9  171    0     6     1     1   \n",
      "6_P    512    78  195  121   56   70  332   22   62   28    21    34    13   \n",
      "7_P   2781    16   33   78   86   29   24  436    9    2    22     8     7   \n",
      "8_P    709    92   33   50   33  360   35   36  260   13    13     5    15   \n",
      "9_P    394   121  223   69   64   34   55   31   35  346    20    25    38   \n",
      "10_P   323    86   65  172   61   35   55   69   21   31   356    22    10   \n",
      "11_P   303   180  552  173   83   27  104   32   36   46    35   150    21   \n",
      "12_P   629   130   59   78   17   33   64   20   29   52    14    10   187   \n",
      "13_P   143   321   49   39   63   13   33   16   28   17     7    20    31   \n",
      "\n",
      "      13_T  \n",
      "0_P      0  \n",
      "1_P     15  \n",
      "2_P      5  \n",
      "3_P      8  \n",
      "4_P     23  \n",
      "5_P      3  \n",
      "6_P     19  \n",
      "7_P      2  \n",
      "8_P      7  \n",
      "9_P     22  \n",
      "10_P     5  \n",
      "11_P    33  \n",
      "12_P    20  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13_P   160  \n",
      "Accuracy (top 1 guesses) - train =  35.382 %\n",
      "\tConfusion Matrix train:\n",
      "        0_T   1_T   2_T   3_T   4_T  5_T  6_T   7_T  8_T  9_T  10_T  11_T  \\\n",
      "0_P   4098     2    17    33     7   14    6    21    1    1     3     1   \n",
      "1_P    219  3873   113    97    32   49   34     9   33   22    14    52   \n",
      "2_P    384   155  1204   275    34    8  160    10   26   41    44   246   \n",
      "3_P   1839   281   278  1747    54   41  183    70   64   26   125    32   \n",
      "4_P   2418   241   252   182  2042  111  339   237  118   79    47    54   \n",
      "5_P    762    58    10    41     5  936   26    20  428    4     4     0   \n",
      "6_P   1169   167   483   237   157  138  884    36  119   37    36    47   \n",
      "7_P   6462    39    66   208   214   69   35  1188   29   10    27     7   \n",
      "8_P   1541   187    72   112    55  899   93    69  668   26    20     9   \n",
      "9_P    900   281   485   164   115   70  130    49   68  946    87    45   \n",
      "10_P   762   187   160   503   127   62   96   141   45   51   918    21   \n",
      "11_P   662   466  1286   383   179   44  245    48   80  142    82   412   \n",
      "12_P  1310   270   129   173    56   85  132    42   64   88    32    16   \n",
      "13_P   324   687   111   101   139   17   65    42   58   58    15    23   \n",
      "\n",
      "      12_T  13_T  \n",
      "0_P      4     0  \n",
      "1_P     38    30  \n",
      "2_P      9     4  \n",
      "3_P     31     7  \n",
      "4_P     22    52  \n",
      "5_P      6     3  \n",
      "6_P     41    19  \n",
      "7_P     11     7  \n",
      "8_P     17    15  \n",
      "9_P     92    41  \n",
      "10_P    12    23  \n",
      "11_P    51    35  \n",
      "12_P   536    32  \n",
      "13_P    57   440  \n"
     ]
    }
   ],
   "source": [
    "print (\"Before training:\")\n",
    "print (\"loss name is: \",loss_name, \" Using loss function: \", loss_fn)\n",
    "accuracy_test(model, X_test, Y_test, data_set_name='test',print_sample = 0, top_n_guess= 1 )\n",
    "accuracy_test(model, X_train, Y_train, data_set_name='train',print_sample = 0, top_n_guess= 1)\n",
    "\n",
    "n_epoch = 3\n",
    "model, optimizer, loss, epoch = train_model(model, n_epoch, dataloader, optimizer, loss_fn, scheduler)\n",
    "\n",
    "print (\"After training:\")\n",
    "print (\"loss name is: \",loss_name, \" Using loss function: \", loss_fn)\n",
    "accuracy_test(model, X_test, Y_test, data_set_name='test',print_sample = 0, top_n_guess= 1 )\n",
    "accuracy_test(model, X_train, Y_train, data_set_name='train',print_sample = 0, top_n_guess= 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(epoch, model, optimizer, loss, scheduler, PATH):\n",
    "    torch.save( {'epoch': 3,\n",
    "                'model': model,\n",
    "                'optimizer': optimizer,\n",
    "                'loss': loss,\n",
    "                'scheduler': scheduler,    \n",
    "                },\n",
    "                PATH)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# running n times, and saving after eveny m epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss < min_loss, updating min_loss to 0.14967256784439087, i_iter 0\n",
      "loss < min_loss, updating min_loss to 0.12891797721385956, i_iter 27\n",
      "loss < min_loss, updating min_loss to 0.06754700094461441, i_iter 33\n",
      "loss < min_loss, updating min_loss to 0.028285078704357147, i_iter 45\n",
      "lr 0.005\n",
      "epoch-1, loss 0.5\n",
      "------------------------------------\n",
      "lr 0.005\n",
      "epoch-2, loss 0.706\n",
      "------------------------------------\n",
      "DONE, returning model\n",
      "After training:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:15: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy (top 1 guesses) - test =  64.193 %\n",
      "\tConfusion Matrix test:\n",
      "       0_T   1_T  2_T  3_T  4_T  5_T\n",
      "0_P  5702     4    1   18    5    1\n",
      "1_P   233  1493   40   22   35   18\n",
      "2_P   268    42  944    7   25   10\n",
      "3_P  1872    50   11  535   64   19\n",
      "4_P   743   218   57  115  295   41\n",
      "5_P  1032    54   18   74   28  219\n",
      "Accuracy (top 1 guesses) - train =  67.064 %\n",
      "\tConfusion Matrix train:\n",
      "        0_T   1_T   2_T   3_T  4_T  5_T\n",
      "0_P  13448     1     3    18    2    4\n",
      "1_P    418  3716    75    31   54   13\n",
      "2_P    643    57  2333    19   33    5\n",
      "3_P   4206    79    29  1466   74   22\n",
      "4_P   1688   437   112   196  856   75\n",
      "5_P   2486    75    28    78   38  577\n",
      "saving the model\n",
      "loss < min_loss, updating min_loss to 0.37200048565864563, i_iter 0\n",
      "loss < min_loss, updating min_loss to 0.15625405311584473, i_iter 13\n",
      "loss < min_loss, updating min_loss to 0.0983743965625763, i_iter 41\n",
      "loss < min_loss, updating min_loss to 0.0924290269613266, i_iter 77\n",
      "loss < min_loss, updating min_loss to 0.0701688900589943, i_iter 103\n",
      "loss < min_loss, updating min_loss to 0.036255624145269394, i_iter 168\n",
      "lr 0.005\n",
      "epoch-1, loss 0.342\n",
      "------------------------------------\n",
      "loss < min_loss, updating min_loss to 0.029322447255253792, i_iter 2115\n",
      "lr 0.005\n",
      "epoch-2, loss 1.007\n",
      "------------------------------------\n",
      "DONE, returning model\n",
      "After training:\n",
      "Accuracy (top 1 guesses) - test =  53.895 %\n",
      "\tConfusion Matrix test:\n",
      "       0_T   1_T  2_T  3_T  4_T  5_T\n",
      "0_P  4299     1    0    6    2    1\n",
      "1_P   122  1418   24   15   25    9\n",
      "2_P   103    46  932    6   12    3\n",
      "3_P  1993    48   19  545   57   15\n",
      "4_P   496   167   36   64  263   23\n",
      "5_P  2837   181   60  135   93  257\n",
      "Accuracy (top 1 guesses) - train =  56.473 %\n",
      "\tConfusion Matrix train:\n",
      "        0_T   1_T   2_T   3_T  4_T  5_T\n",
      "0_P  10127     1     1     6    0    0\n",
      "1_P    231  3534    37    17   31    4\n",
      "2_P    226    68  2322     4   23    5\n",
      "3_P   4569    94    39  1451   82   18\n",
      "4_P   1181   346    73   137  790   34\n",
      "5_P   6555   322   108   193  131  635\n",
      "saving the model\n",
      "loss < min_loss, updating min_loss to 0.7474266886711121, i_iter 0\n",
      "loss < min_loss, updating min_loss to 0.6367447972297668, i_iter 1\n",
      "loss < min_loss, updating min_loss to 0.6036290526390076, i_iter 4\n",
      "loss < min_loss, updating min_loss to 0.2552180290222168, i_iter 7\n",
      "loss < min_loss, updating min_loss to 0.23288969695568085, i_iter 10\n",
      "loss < min_loss, updating min_loss to 0.2092178910970688, i_iter 13\n",
      "loss < min_loss, updating min_loss to 0.1593487560749054, i_iter 19\n",
      "loss < min_loss, updating min_loss to 0.13969142735004425, i_iter 40\n",
      "loss < min_loss, updating min_loss to 0.07911290228366852, i_iter 84\n",
      "loss < min_loss, updating min_loss to 0.05951814725995064, i_iter 397\n",
      "loss < min_loss, updating min_loss to 0.05730752274394035, i_iter 718\n",
      "loss < min_loss, updating min_loss to 0.05612122267484665, i_iter 773\n",
      "loss < min_loss, updating min_loss to 0.050885170698165894, i_iter 836\n",
      "loss < min_loss, updating min_loss to 0.04586710408329964, i_iter 890\n",
      "lr 0.005\n",
      "epoch-1, loss 0.789\n",
      "------------------------------------\n",
      "loss < min_loss, updating min_loss to 0.02676081284880638, i_iter 2033\n",
      "lr 0.005\n",
      "epoch-2, loss 0.245\n",
      "------------------------------------\n",
      "DONE, returning model\n",
      "After training:\n",
      "Accuracy (top 1 guesses) - test =  70.356 %\n",
      "\tConfusion Matrix test:\n",
      "       0_T   1_T  2_T  3_T  4_T  5_T\n",
      "0_P  6626    17    9   52   13   11\n",
      "1_P   113  1433   20    9   18   14\n",
      "2_P   102    36  943    9    7    9\n",
      "3_P  1629    44    9  511   42   18\n",
      "4_P   664   279   71  126  340   39\n",
      "5_P   716    52   19   64   32  217\n",
      "Accuracy (top 1 guesses) - train =  72.885 %\n",
      "\tConfusion Matrix train:\n",
      "        0_T   1_T   2_T   3_T  4_T  5_T\n",
      "0_P  15523    15    21    35    3   10\n",
      "1_P    270  3525    33    14   17    7\n",
      "2_P    240    45  2323     7   15    4\n",
      "3_P   3655    77    31  1445   53   16\n",
      "4_P   1552   609   139   225  933   68\n",
      "5_P   1649    94    33    82   36  591\n",
      "saving the model\n",
      "loss < min_loss, updating min_loss to 0.3015149235725403, i_iter 0\n",
      "loss < min_loss, updating min_loss to 0.2936626970767975, i_iter 4\n",
      "loss < min_loss, updating min_loss to 0.27534419298171997, i_iter 9\n",
      "loss < min_loss, updating min_loss to 0.23459941148757935, i_iter 17\n",
      "loss < min_loss, updating min_loss to 0.20289361476898193, i_iter 20\n",
      "loss < min_loss, updating min_loss to 0.12724991142749786, i_iter 22\n",
      "loss < min_loss, updating min_loss to 0.08660198748111725, i_iter 24\n",
      "loss < min_loss, updating min_loss to 0.08572466671466827, i_iter 37\n",
      "loss < min_loss, updating min_loss to 0.06633694469928741, i_iter 79\n",
      "loss < min_loss, updating min_loss to 0.047269176691770554, i_iter 141\n",
      "loss < min_loss, updating min_loss to 0.03903500363230705, i_iter 148\n",
      "loss < min_loss, updating min_loss to 0.026532413437962532, i_iter 578\n",
      "lr 0.005\n",
      "epoch-1, loss 0.806\n",
      "------------------------------------\n",
      "loss < min_loss, updating min_loss to 0.02367141842842102, i_iter 3685\n",
      "lr 0.005\n",
      "epoch-2, loss 1.474\n",
      "------------------------------------\n",
      "DONE, returning model\n",
      "After training:\n",
      "Accuracy (top 1 guesses) - test =  69.154 %\n",
      "\tConfusion Matrix test:\n",
      "       0_T   1_T  2_T  3_T  4_T  5_T\n",
      "0_P  6483    16    7   51    8   15\n",
      "1_P    93  1415   21    9   23    9\n",
      "2_P   164    49  945   10   12   12\n",
      "3_P  1897    61   19  547   66   28\n",
      "4_P   671   281   73  118  321   57\n",
      "5_P   542    39    6   36   22  187\n",
      "Accuracy (top 1 guesses) - train =  71.277 %\n",
      "\tConfusion Matrix train:\n",
      "        0_T   1_T   2_T   3_T  4_T  5_T\n",
      "0_P  15014    15    12    39    4   10\n",
      "1_P    258  3506    45    18   17   13\n",
      "2_P    382    98  2327    17   17    1\n",
      "3_P   4471    87    35  1480   68   30\n",
      "4_P   1461   599   141   205  931   97\n",
      "5_P   1303    60    20    49   20  545\n",
      "saving the model\n",
      "loss < min_loss, updating min_loss to 0.9526494741439819, i_iter 0\n",
      "loss < min_loss, updating min_loss to 0.1763814240694046, i_iter 1\n",
      "loss < min_loss, updating min_loss to 0.1724790781736374, i_iter 8\n",
      "loss < min_loss, updating min_loss to 0.12044574320316315, i_iter 11\n",
      "loss < min_loss, updating min_loss to 0.0871555283665657, i_iter 86\n",
      "loss < min_loss, updating min_loss to 0.05833420529961586, i_iter 212\n",
      "loss < min_loss, updating min_loss to 0.04979878291487694, i_iter 262\n",
      "loss < min_loss, updating min_loss to 0.030779872089624405, i_iter 292\n",
      "lr 0.005\n",
      "epoch-1, loss 0.533\n",
      "------------------------------------\n",
      "lr 0.005\n",
      "epoch-2, loss 1.015\n",
      "------------------------------------\n",
      "DONE, returning model\n",
      "After training:\n",
      "Accuracy (top 1 guesses) - test =  72.41 %\n",
      "\tConfusion Matrix test:\n",
      "       0_T   1_T  2_T  3_T  4_T  5_T\n",
      "0_P  6919    13    6   84   15   15\n",
      "1_P   179  1499   31   31   38   14\n",
      "2_P   327    63  961   17   22   15\n",
      "3_P  1045    26   10  460   35   14\n",
      "4_P   657   207   49  113  314   39\n",
      "5_P   723    53   14   66   28  211\n",
      "Accuracy (top 1 guesses) - train =  75.577 %\n",
      "\tConfusion Matrix train:\n",
      "        0_T   1_T   2_T   3_T  4_T  5_T\n",
      "0_P  16262    11     6    83    2    6\n",
      "1_P    412  3707    46    29   31    8\n",
      "2_P    722    98  2380    20   36    7\n",
      "3_P   2389    37    14  1355   33   11\n",
      "4_P   1481   426   116   245  931   60\n",
      "5_P   1623    86    18    76   24  604\n",
      "saving the model\n"
     ]
    }
   ],
   "source": [
    "PATH_TEMPLATE = \"saved_models\\model_{}.pt\"\n",
    "\n",
    "epoch_per_iter = 10\n",
    "iters = 10\n",
    "for iter in range(iters):\n",
    "    model, optimizer, loss, epoch = train_model(model, epoch_per_iter, dataloader, optimizer, loss_fn, scheduler)\n",
    "    \n",
    "    print (\"After training:\")\n",
    "    accuracy_test(model, X_test, Y_test, data_set_name='test',print_sample = 0, top_n_guess= 1 )\n",
    "    accuracy_test(model, X_train, Y_train, data_set_name='train',print_sample = 0, top_n_guess= 1)\n",
    "\n",
    "    print (\"saving the model\")\n",
    "    save_model(epoch, model, optimizer, loss, scheduler, PATH_TEMPLATE.format(iter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = torch.load(PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'epoch': 3, 'model': Sequential(\n",
       "   (0): Linear(in_features=2000, out_features=100, bias=True)\n",
       "   (1): ReLU()\n",
       "   (2): Linear(in_features=100, out_features=70, bias=True)\n",
       "   (3): ReLU()\n",
       "   (4): Linear(in_features=70, out_features=50, bias=True)\n",
       "   (5): ReLU()\n",
       "   (6): Dropout(p=0.5, inplace=False)\n",
       "   (7): Linear(in_features=50, out_features=14, bias=True)\n",
       " ), 'optimizer': Adam (\n",
       " Parameter Group 0\n",
       "     amsgrad: False\n",
       "     betas: (0.9, 0.999)\n",
       "     eps: 1e-08\n",
       "     lr: 0.005\n",
       "     weight_decay: 0.001\n",
       " ), 'loss': tensor(3.2718, requires_grad=True), 'scheduler': <torch.optim.lr_scheduler.ReduceLROnPlateau at 0x1a294e50ef0>}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<enumerate at 0x1a2c66fb3f0>"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enumerate(torch_data.TensorDataset(X_train,torch.tensor(Y_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
