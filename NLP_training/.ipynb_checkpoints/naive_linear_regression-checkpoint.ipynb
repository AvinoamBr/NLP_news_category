{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\אבינעם\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\אבינעם\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\אבינעם\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import nltk\n",
    "import sklearn\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import re\n",
    "import torch.utils.data as torch_data\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "# from Utils.pytorch_utils import torch_net\n",
    "from Utils.pytorch_utils import sparse_to_matrix #, accuracy_test\n",
    "\n",
    "from Utils.NLP_utils import accuracy, find_senteces_with_lemma, get_wordnet_pos, load_and_lemmatize_data, load_processed_data\n",
    "\n",
    "# pickle file, data set as readable json file, since original data set is a 'pseudo json', written in text file.\n",
    "DATA_SET_FILE = r\"C:\\Users\\גורים\\PycharmProjects\\NLP_training\\datasets\\News_Category_Dataset_v2_mod.pkl\"\n",
    "PROCESSED_DATA_SET = r\"C:\\Users\\גורים\\PycharmProjects\\NLP_training\\datasets\\News_Category_Dataset_v2_mod_processed.pkl\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def torch_net(X_in, Y_in, X_test, Y_test,\n",
    "              hidden_layers=[10], device=torch.device('cpu'), epoch=30, batch_size=10):\n",
    "    \n",
    "    def set_learning_rate(optimizer,lr):\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = lr\n",
    "            \n",
    "    def print_learning_rate(optimizer):\n",
    "        for param_group in optimizer.param_groups:\n",
    "            print(param_group['lr'])\n",
    "    \n",
    "    # hiden_layers = [size1,size2...]\n",
    "\n",
    "    dtype = torch.float\n",
    "    # device = torch.device(\"cuda:0\") # Uncomment this to run on GPU\n",
    "\n",
    "    # N is batch size; D_in is input dimension;\n",
    "    # H is hidden dimension; D_out is output dimension.\n",
    "    N, D_in = X_in.shape\n",
    "    D_out = Y_in.shape[-1]\n",
    "\n",
    "    # TODO - support multiple layers\n",
    "    H = hidden_layers[0]\n",
    "    # N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "    # Create random input and output data\n",
    "\n",
    "    [X_in, Y_in, X_test, Y_test] = \\\n",
    "        [sparse_to_matrix(A) for A in[X_in, Y_in, X_test, Y_test]]\n",
    "\n",
    "    X = torch.tensor(X_in, device=device, dtype=dtype)\n",
    "    Y = torch.tensor(Y_in, device=device, dtype=dtype)\n",
    "\n",
    "    # Use the nn package to define our model and loss function.\n",
    "    \n",
    "#     model = torch.nn.Sequential(\n",
    "#         torch.nn.Linear(D_in, H),\n",
    "#         torch.nn.ReLU(),\n",
    "#         torch.nn.Linear(H, D_out),\n",
    "#     )\n",
    "    \n",
    "    #create neural network net with multiple hidden layers with H dimetions:\n",
    "    dims = [D_in, *hidden_layers, D_out]\n",
    "    layers = []\n",
    "    for dim_ind in range(len(dims)-2):\n",
    "        layers.append(torch.nn.Linear(dims[dim_ind], dims[dim_ind+1]))\n",
    "        layers.append(torch.nn.ReLU())\n",
    "    layers.append(torch.nn.Dropout(0.5))\n",
    "    layers.append(torch.nn.Linear(hidden_layers[-1], D_out))\n",
    "    \n",
    "    model = torch.nn.Sequential(*layers)\n",
    "    loss_fn = torch.nn.MSELoss(reduction='sum')\n",
    "\n",
    "    # Use the optim package to define an Optimizer that will update the weights of\n",
    "    # the model for us. Here we will use Adam; the optim package contains many other\n",
    "    # optimization algoriths. The first argument to the Adam constructor tells the\n",
    "    # optimizer which Tensors it should update.\n",
    "    learning_rate = 0.05\n",
    "    weight_decay = 0.99\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "\n",
    "    dataloader = torch_data.DataLoader(\n",
    "        torch_data.TensorDataset(X, Y), batch_size=batch_size,\n",
    "        shuffle=True, num_workers=4)\n",
    "    print (\"Before training:\")\n",
    "    accuracy_test(model, X_test, Y_test, data_set_name='test',print_sample = 16)\n",
    "    accuracy_test(model, X_in, Y_in, data_set_name='train',print_sample = 16)\n",
    "    \n",
    "    epoch_lr = learning_rate\n",
    "    for e in range(epoch):\n",
    "        print(f\"\\t +++ epoch: {e+1} +++\")\n",
    "        for t,(x_batch, y_batch) in enumerate(dataloader):\n",
    "            # Forward pass: compute predicted y by passing x to the model.\n",
    "            y_pred = model(x_batch)\n",
    "\n",
    "            # Compute and print loss.\n",
    "            loss = loss_fn(y_pred, y_batch)\n",
    "            if not ( (t +1) % 2000 ) :\n",
    "                print(f\"iter-{t+1}, loss {round(loss.item(),3)}\")\n",
    "\n",
    "            # Before the backward pass, use the optimizer object to zero all of the\n",
    "            # gradients for the variables it will update (which are the learnable\n",
    "            # weights of the model). This is because by default, gradients are\n",
    "            # accumulated in buffers( i.e, not overwritten) whenever .backward()\n",
    "            # is called. Checkout docs of torch.autograd.backward for more details.\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Backward pass: compute gradient of the loss with respect to model\n",
    "            # parameters\n",
    "\n",
    "            #  $$$ this command destroy exit() command $$$\n",
    "            loss.backward()\n",
    "\n",
    "            # Calling the step function on an Optimizer makes an update to its\n",
    "            # parameters\n",
    "            optimizer.step()\n",
    "        accuracy_test(model, X_test, Y_test, data_set_name= 'test', print_sample=16)\n",
    "        accuracy_test(model, X_in, Y_in, data_set_name= 'train', print_sample = 16)\n",
    "        \n",
    "        epoch_lr = epoch_lr * 0.5\n",
    "        set_learning_rate(optimizer,epoch_lr)\n",
    "        print (\"printing learning rate after update:\")\n",
    "        print_learning_rate(optimizer)\n",
    "        \n",
    "    print (\"DONE, returning model\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_test(model, x, y, data_set_name = 'test',print_sample = False):\n",
    "    predicted = torch.argmax(model(torch.tensor(x, dtype=torch.float)), dim=-1).numpy()\n",
    "    truth = np.argmax(y, axis=-1)\n",
    "    # print(np.array((predicted, truth)))\n",
    "    print (f\"Accuracy {data_set_name} = \",\n",
    "           round( np.array(predicted == truth).mean()* 100, 3 ),\n",
    "           \"%\")\n",
    "    if print_sample:\n",
    "        ps = print_sample\n",
    "        sample = pd.DataFrame((predicted[:ps],truth[:ps]),index=['predicted','truth'])\n",
    "        print (sample)\n",
    "        print (pd.value_counts(predicted))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading data\n",
    "\n",
    "# dataset, headlines, headlines_orig = load_and_lemmatize_data(DATA_SET_FILE)\n",
    "dataset, headlines, headlines_orig = load_processed_data(PROCESSED_DATA_SET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = dataset['category']\n",
    "Y  = pd.get_dummies(categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data and lables to train/test\n",
    "\n",
    "headlines_train, headlines_test,\\\n",
    "headlines_train_orig, headlines_test_orig,\\\n",
    "Y_train, Y_test,\\\n",
    "cat_train, cat_test\\\n",
    "    = sklearn.model_selection.train_test_split(\n",
    "    headlines,headlines_orig, Y, categories, test_size = 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract features (Bag Of Words) using Vectorizer\n",
    "\n",
    "max_features=1000\n",
    "\n",
    "vectorizer = CountVectorizer\n",
    "# vectorizer = TfidfVectorizer\n",
    "matrix = vectorizer(max_features=max_features, ngram_range=(1, 2), max_df=0.1 ,min_df = 5)\n",
    "matrix.fit(headlines_train)\n",
    "X_train = matrix.transform(headlines_train)# .todense()\n",
    "X_test = matrix.transform(headlines_test)# .todense()\n",
    "\n",
    "# --- convert to data frame for display and debug ---\n",
    "# tokens = matrix.get_feature_names()\n",
    "# X_train= pd.DataFrame(X_train,columns=tokens)\n",
    "# X_test= pd.DataFrame(X_test,columns=tokens)\n",
    "\n",
    "assert X_train.shape[1]==max_features, X_train.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before training:\n",
      "Accuracy test =  2.352 %\n",
      "           0   1   2   3   4   5   6   7   8   9   10  11  12  13  14  15\n",
      "predicted   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2\n",
      "truth      13  38  32   0   6  24  34  37  26  24  15  24  24  24  24  24\n",
      "2     56416\n",
      "37     2564\n",
      "7       373\n",
      "27      320\n",
      "4       312\n",
      "30      157\n",
      "35       92\n",
      "23        8\n",
      "11        8\n",
      "0         4\n",
      "14        1\n",
      "39        1\n",
      "dtype: int64\n",
      "Accuracy train =  2.433 %\n",
      "           0   1   2   3   4   5   6   7   8   9   10  11  12  13  14  15\n",
      "predicted   2   2   7   2   2   2   2  27   2   2   2   2   2   2   2   2\n",
      "truth      24  40  10  39  14   5   3   6  34  24  24  24  10  37  17  24\n",
      "2     131335\n",
      "37      6131\n",
      "7        884\n",
      "4        793\n",
      "27       787\n",
      "30       362\n",
      "35       207\n",
      "23        42\n",
      "11        39\n",
      "14         9\n",
      "0          5\n",
      "39         3\n",
      "dtype: int64\n",
      "\t +++ epoch: 1 +++\n",
      "iter-2000, loss 9.468\n",
      "iter-4000, loss 10.377\n",
      "iter-6000, loss 9.77\n",
      "iter-8000, loss 10.041\n",
      "iter-10000, loss 9.569\n",
      "iter-12000, loss 9.362\n",
      "iter-14000, loss 9.199\n",
      "Accuracy test =  16.476 %\n",
      "           0   1   2   3   4   5   6   7   8   9   10  11  12  13  14  15\n",
      "predicted  24  24  24  24  24  24  24  24  24  24  24  24  24  24  24  24\n",
      "truth      13  38  32   0   6  24  34  37  26  24  15  24  24  24  24  24\n",
      "24    60254\n",
      "37        2\n",
      "dtype: int64\n",
      "Accuracy train =  16.227 %\n",
      "           0   1   2   3   4   5   6   7   8   9   10  11  12  13  14  15\n",
      "predicted  24  24  24  24  24  24  24  24  24  24  24  24  24  24  24  24\n",
      "truth      24  40  10  39  14   5   3   6  34  24  24  24  10  37  17  24\n",
      "24    140585\n",
      "37         9\n",
      "13         3\n",
      "dtype: int64\n",
      "printing learning rate after update:\n",
      "0.025\n",
      "\t +++ epoch: 2 +++\n",
      "iter-2000, loss 9.27\n",
      "iter-4000, loss 9.816\n",
      "iter-6000, loss 9.951\n",
      "iter-8000, loss 9.427\n",
      "iter-10000, loss 9.32\n",
      "iter-12000, loss 9.635\n",
      "iter-14000, loss 9.378\n",
      "Accuracy test =  15.822 %\n",
      "           0   1   2   3   4   5   6   7   8   9   10  11  12  13  14  15\n",
      "predicted  37  37  24  37  37  24  37  24  37  24  37  24  24  24  24  24\n",
      "truth      13  38  32   0   6  24  34  37  26  24  15  24  24  24  24  24\n",
      "37    28248\n",
      "24    26490\n",
      "34     5518\n",
      "dtype: int64\n",
      "Accuracy train =  15.674 %\n",
      "           0   1   2   3   4   5   6   7   8   9   10  11  12  13  14  15\n",
      "predicted  24  37  24  37  37  37  24  34  37  24  24  37  37  37  37  24\n",
      "truth      24  40  10  39  14   5   3   6  34  24  24  24  10  37  17  24\n",
      "37    65877\n",
      "24    61775\n",
      "34    12945\n",
      "dtype: int64\n",
      "printing learning rate after update:\n",
      "0.0125\n",
      "\t +++ epoch: 3 +++\n",
      "iter-2000, loss 9.941\n",
      "iter-4000, loss 9.163\n",
      "iter-6000, loss 8.609\n",
      "iter-8000, loss 9.374\n",
      "iter-10000, loss 9.386\n",
      "iter-12000, loss 9.412\n",
      "iter-14000, loss 9.252\n",
      "Accuracy test =  16.475 %\n",
      "           0   1   2   3   4   5   6   7   8   9   10  11  12  13  14  15\n",
      "predicted  24  24  24  24  24  24  24  24  24  24  24  24  24  24  24  24\n",
      "truth      13  38  32   0   6  24  34  37  26  24  15  24  24  24  24  24\n",
      "24    60255\n",
      "37        1\n",
      "dtype: int64\n",
      "Accuracy train =  16.224 %\n",
      "           0   1   2   3   4   5   6   7   8   9   10  11  12  13  14  15\n",
      "predicted  24  24  24  24  24  24  24  24  24  24  24  24  24  24  24  24\n",
      "truth      24  40  10  39  14   5   3   6  34  24  24  24  10  37  17  24\n",
      "24    140597\n",
      "dtype: int64\n",
      "printing learning rate after update:\n",
      "0.00625\n",
      "\t +++ epoch: 4 +++\n",
      "iter-2000, loss 9.496\n",
      "iter-4000, loss 9.623\n",
      "iter-6000, loss 9.425\n",
      "iter-8000, loss 9.369\n",
      "iter-10000, loss 9.528\n",
      "iter-12000, loss 9.611\n",
      "iter-14000, loss 9.704\n",
      "Accuracy test =  16.476 %\n",
      "           0   1   2   3   4   5   6   7   8   9   10  11  12  13  14  15\n",
      "predicted  24  24  24  24  24  24  24  24  24  24  24  24  24  24  24  24\n",
      "truth      13  38  32   0   6  24  34  37  26  24  15  24  24  24  24  24\n",
      "24    60256\n",
      "dtype: int64\n",
      "Accuracy train =  16.224 %\n",
      "           0   1   2   3   4   5   6   7   8   9   10  11  12  13  14  15\n",
      "predicted  24  24  24  24  24  24  24  24  24  24  24  24  24  24  24  24\n",
      "truth      24  40  10  39  14   5   3   6  34  24  24  24  10  37  17  24\n",
      "24    140597\n",
      "dtype: int64\n",
      "printing learning rate after update:\n",
      "0.003125\n",
      "\t +++ epoch: 5 +++\n",
      "iter-2000, loss 9.799\n",
      "iter-4000, loss 9.102\n",
      "iter-6000, loss 9.801\n",
      "iter-8000, loss 9.615\n",
      "iter-10000, loss 10.204\n",
      "iter-12000, loss 9.718\n",
      "iter-14000, loss 8.703\n",
      "Accuracy test =  16.476 %\n",
      "           0   1   2   3   4   5   6   7   8   9   10  11  12  13  14  15\n",
      "predicted  24  24  24  24  24  24  24  24  24  24  24  24  24  24  24  24\n",
      "truth      13  38  32   0   6  24  34  37  26  24  15  24  24  24  24  24\n",
      "24    60256\n",
      "dtype: int64\n",
      "Accuracy train =  16.224 %\n",
      "           0   1   2   3   4   5   6   7   8   9   10  11  12  13  14  15\n",
      "predicted  24  24  24  24  24  24  24  24  24  24  24  24  24  24  24  24\n",
      "truth      24  40  10  39  14   5   3   6  34  24  24  24  10  37  17  24\n",
      "24    140597\n",
      "dtype: int64\n",
      "printing learning rate after update:\n",
      "0.0015625\n",
      "\t +++ epoch: 6 +++\n",
      "iter-2000, loss 9.175\n",
      "iter-4000, loss 9.627\n",
      "iter-6000, loss 9.742\n",
      "iter-8000, loss 9.376\n",
      "iter-10000, loss 9.128\n",
      "iter-12000, loss 9.854\n",
      "iter-14000, loss 9.478\n",
      "Accuracy test =  16.476 %\n",
      "           0   1   2   3   4   5   6   7   8   9   10  11  12  13  14  15\n",
      "predicted  24  24  24  24  24  24  24  24  24  24  24  24  24  24  24  24\n",
      "truth      13  38  32   0   6  24  34  37  26  24  15  24  24  24  24  24\n",
      "24    60256\n",
      "dtype: int64\n",
      "Accuracy train =  16.224 %\n",
      "           0   1   2   3   4   5   6   7   8   9   10  11  12  13  14  15\n",
      "predicted  24  24  24  24  24  24  24  24  24  24  24  24  24  24  24  24\n",
      "truth      24  40  10  39  14   5   3   6  34  24  24  24  10  37  17  24\n",
      "24    140597\n",
      "dtype: int64\n",
      "printing learning rate after update:\n",
      "0.00078125\n",
      "\t +++ epoch: 7 +++\n",
      "iter-2000, loss 9.742\n",
      "iter-4000, loss 9.649\n",
      "iter-6000, loss 9.295\n",
      "iter-8000, loss 9.613\n",
      "iter-10000, loss 9.354\n",
      "iter-12000, loss 9.38\n",
      "iter-14000, loss 9.336\n",
      "Accuracy test =  16.476 %\n",
      "           0   1   2   3   4   5   6   7   8   9   10  11  12  13  14  15\n",
      "predicted  24  24  24  24  24  24  24  24  24  24  24  24  24  24  24  24\n",
      "truth      13  38  32   0   6  24  34  37  26  24  15  24  24  24  24  24\n",
      "24    60256\n",
      "dtype: int64\n",
      "Accuracy train =  16.224 %\n",
      "           0   1   2   3   4   5   6   7   8   9   10  11  12  13  14  15\n",
      "predicted  24  24  24  24  24  24  24  24  24  24  24  24  24  24  24  24\n",
      "truth      24  40  10  39  14   5   3   6  34  24  24  24  10  37  17  24\n",
      "24    140597\n",
      "dtype: int64\n",
      "printing learning rate after update:\n",
      "0.000390625\n",
      "\t +++ epoch: 8 +++\n",
      "iter-2000, loss 9.325\n",
      "iter-4000, loss 9.517\n",
      "iter-6000, loss 9.719\n",
      "iter-8000, loss 9.202\n",
      "iter-10000, loss 8.737\n",
      "iter-12000, loss 9.512\n",
      "iter-14000, loss 9.594\n",
      "Accuracy test =  16.476 %\n",
      "           0   1   2   3   4   5   6   7   8   9   10  11  12  13  14  15\n",
      "predicted  24  24  24  24  24  24  24  24  24  24  24  24  24  24  24  24\n",
      "truth      13  38  32   0   6  24  34  37  26  24  15  24  24  24  24  24\n",
      "24    60256\n",
      "dtype: int64\n",
      "Accuracy train =  16.224 %\n",
      "           0   1   2   3   4   5   6   7   8   9   10  11  12  13  14  15\n",
      "predicted  24  24  24  24  24  24  24  24  24  24  24  24  24  24  24  24\n",
      "truth      24  40  10  39  14   5   3   6  34  24  24  24  10  37  17  24\n",
      "24    140597\n",
      "dtype: int64\n",
      "printing learning rate after update:\n",
      "0.0001953125\n",
      "\t +++ epoch: 9 +++\n",
      "iter-2000, loss 9.693\n",
      "iter-4000, loss 9.709\n",
      "iter-6000, loss 9.554\n",
      "iter-8000, loss 9.779\n",
      "iter-10000, loss 9.252\n",
      "iter-12000, loss 8.802\n",
      "iter-14000, loss 9.466\n",
      "Accuracy test =  16.476 %\n",
      "           0   1   2   3   4   5   6   7   8   9   10  11  12  13  14  15\n",
      "predicted  24  24  24  24  24  24  24  24  24  24  24  24  24  24  24  24\n",
      "truth      13  38  32   0   6  24  34  37  26  24  15  24  24  24  24  24\n",
      "24    60256\n",
      "dtype: int64\n",
      "Accuracy train =  16.224 %\n",
      "           0   1   2   3   4   5   6   7   8   9   10  11  12  13  14  15\n",
      "predicted  24  24  24  24  24  24  24  24  24  24  24  24  24  24  24  24\n",
      "truth      24  40  10  39  14   5   3   6  34  24  24  24  10  37  17  24\n",
      "24    140597\n",
      "dtype: int64\n",
      "printing learning rate after update:\n",
      "9.765625e-05\n",
      "\t +++ epoch: 10 +++\n",
      "iter-2000, loss 9.329\n",
      "iter-4000, loss 9.402\n",
      "iter-6000, loss 9.526\n",
      "iter-8000, loss 9.211\n",
      "iter-10000, loss 9.564\n",
      "iter-12000, loss 9.186\n",
      "iter-14000, loss 9.293\n",
      "Accuracy test =  16.476 %\n",
      "           0   1   2   3   4   5   6   7   8   9   10  11  12  13  14  15\n",
      "predicted  24  24  24  24  24  24  24  24  24  24  24  24  24  24  24  24\n",
      "truth      13  38  32   0   6  24  34  37  26  24  15  24  24  24  24  24\n",
      "24    60256\n",
      "dtype: int64\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy train =  16.224 %\n",
      "           0   1   2   3   4   5   6   7   8   9   10  11  12  13  14  15\n",
      "predicted  24  24  24  24  24  24  24  24  24  24  24  24  24  24  24  24\n",
      "truth      24  40  10  39  14   5   3   6  34  24  24  24  10  37  17  24\n",
      "24    140597\n",
      "dtype: int64\n",
      "printing learning rate after update:\n",
      "4.8828125e-05\n",
      "\t +++ epoch: 11 +++\n",
      "iter-2000, loss 9.494\n",
      "iter-4000, loss 9.426\n",
      "iter-6000, loss 9.735\n",
      "iter-8000, loss 9.339\n",
      "iter-10000, loss 9.759\n",
      "iter-12000, loss 8.988\n",
      "iter-14000, loss 9.454\n",
      "Accuracy test =  16.476 %\n",
      "           0   1   2   3   4   5   6   7   8   9   10  11  12  13  14  15\n",
      "predicted  24  24  24  24  24  24  24  24  24  24  24  24  24  24  24  24\n",
      "truth      13  38  32   0   6  24  34  37  26  24  15  24  24  24  24  24\n",
      "24    60256\n",
      "dtype: int64\n",
      "Accuracy train =  16.224 %\n",
      "           0   1   2   3   4   5   6   7   8   9   10  11  12  13  14  15\n",
      "predicted  24  24  24  24  24  24  24  24  24  24  24  24  24  24  24  24\n",
      "truth      24  40  10  39  14   5   3   6  34  24  24  24  10  37  17  24\n",
      "24    140597\n",
      "dtype: int64\n",
      "printing learning rate after update:\n",
      "2.44140625e-05\n",
      "\t +++ epoch: 12 +++\n",
      "iter-2000, loss 9.112\n",
      "iter-4000, loss 9.518\n",
      "iter-6000, loss 9.366\n",
      "iter-8000, loss 9.568\n",
      "iter-10000, loss 9.415\n",
      "iter-12000, loss 9.673\n",
      "iter-14000, loss 9.302\n",
      "Accuracy test =  16.476 %\n",
      "           0   1   2   3   4   5   6   7   8   9   10  11  12  13  14  15\n",
      "predicted  24  24  24  24  24  24  24  24  24  24  24  24  24  24  24  24\n",
      "truth      13  38  32   0   6  24  34  37  26  24  15  24  24  24  24  24\n",
      "24    60256\n",
      "dtype: int64\n",
      "Accuracy train =  16.224 %\n",
      "           0   1   2   3   4   5   6   7   8   9   10  11  12  13  14  15\n",
      "predicted  24  24  24  24  24  24  24  24  24  24  24  24  24  24  24  24\n",
      "truth      24  40  10  39  14   5   3   6  34  24  24  24  10  37  17  24\n",
      "24    140597\n",
      "dtype: int64\n",
      "printing learning rate after update:\n",
      "1.220703125e-05\n",
      "\t +++ epoch: 13 +++\n",
      "iter-2000, loss 8.767\n",
      "iter-4000, loss 9.528\n",
      "iter-6000, loss 9.15\n",
      "iter-8000, loss 9.676\n",
      "iter-10000, loss 9.135\n",
      "iter-12000, loss 9.243\n",
      "iter-14000, loss 9.105\n",
      "Accuracy test =  16.476 %\n",
      "           0   1   2   3   4   5   6   7   8   9   10  11  12  13  14  15\n",
      "predicted  24  24  24  24  24  24  24  24  24  24  24  24  24  24  24  24\n",
      "truth      13  38  32   0   6  24  34  37  26  24  15  24  24  24  24  24\n",
      "24    60256\n",
      "dtype: int64\n",
      "Accuracy train =  16.224 %\n",
      "           0   1   2   3   4   5   6   7   8   9   10  11  12  13  14  15\n",
      "predicted  24  24  24  24  24  24  24  24  24  24  24  24  24  24  24  24\n",
      "truth      24  40  10  39  14   5   3   6  34  24  24  24  10  37  17  24\n",
      "24    140597\n",
      "dtype: int64\n",
      "printing learning rate after update:\n",
      "6.103515625e-06\n",
      "\t +++ epoch: 14 +++\n",
      "iter-2000, loss 9.161\n",
      "iter-4000, loss 9.368\n",
      "iter-6000, loss 9.012\n",
      "iter-8000, loss 9.29\n",
      "iter-10000, loss 8.949\n",
      "iter-12000, loss 8.914\n",
      "iter-14000, loss 9.09\n",
      "Accuracy test =  16.476 %\n",
      "           0   1   2   3   4   5   6   7   8   9   10  11  12  13  14  15\n",
      "predicted  24  24  24  24  24  24  24  24  24  24  24  24  24  24  24  24\n",
      "truth      13  38  32   0   6  24  34  37  26  24  15  24  24  24  24  24\n",
      "24    60256\n",
      "dtype: int64\n",
      "Accuracy train =  16.224 %\n",
      "           0   1   2   3   4   5   6   7   8   9   10  11  12  13  14  15\n",
      "predicted  24  24  24  24  24  24  24  24  24  24  24  24  24  24  24  24\n",
      "truth      24  40  10  39  14   5   3   6  34  24  24  24  10  37  17  24\n",
      "24    140597\n",
      "dtype: int64\n",
      "printing learning rate after update:\n",
      "3.0517578125e-06\n",
      "\t +++ epoch: 15 +++\n",
      "iter-2000, loss 9.175\n",
      "iter-4000, loss 9.351\n",
      "iter-6000, loss 9.766\n",
      "iter-8000, loss 9.181\n",
      "iter-10000, loss 9.154\n",
      "iter-12000, loss 9.404\n",
      "iter-14000, loss 9.534\n",
      "Accuracy test =  16.476 %\n",
      "           0   1   2   3   4   5   6   7   8   9   10  11  12  13  14  15\n",
      "predicted  24  24  24  24  24  24  24  24  24  24  24  24  24  24  24  24\n",
      "truth      13  38  32   0   6  24  34  37  26  24  15  24  24  24  24  24\n",
      "24    60256\n",
      "dtype: int64\n",
      "Accuracy train =  16.224 %\n",
      "           0   1   2   3   4   5   6   7   8   9   10  11  12  13  14  15\n",
      "predicted  24  24  24  24  24  24  24  24  24  24  24  24  24  24  24  24\n",
      "truth      24  40  10  39  14   5   3   6  34  24  24  24  10  37  17  24\n",
      "24    140597\n",
      "dtype: int64\n",
      "printing learning rate after update:\n",
      "1.52587890625e-06\n",
      "\t +++ epoch: 16 +++\n",
      "iter-2000, loss 9.113\n",
      "iter-4000, loss 9.621\n",
      "iter-6000, loss 9.25\n",
      "iter-8000, loss 9.246\n",
      "iter-10000, loss 9.078\n",
      "iter-12000, loss 9.382\n",
      "iter-14000, loss 9.12\n",
      "Accuracy test =  16.476 %\n",
      "           0   1   2   3   4   5   6   7   8   9   10  11  12  13  14  15\n",
      "predicted  24  24  24  24  24  24  24  24  24  24  24  24  24  24  24  24\n",
      "truth      13  38  32   0   6  24  34  37  26  24  15  24  24  24  24  24\n",
      "24    60256\n",
      "dtype: int64\n",
      "Accuracy train =  16.224 %\n",
      "           0   1   2   3   4   5   6   7   8   9   10  11  12  13  14  15\n",
      "predicted  24  24  24  24  24  24  24  24  24  24  24  24  24  24  24  24\n",
      "truth      24  40  10  39  14   5   3   6  34  24  24  24  10  37  17  24\n",
      "24    140597\n",
      "dtype: int64\n",
      "printing learning rate after update:\n",
      "7.62939453125e-07\n",
      "\t +++ epoch: 17 +++\n",
      "iter-2000, loss 9.919\n",
      "iter-4000, loss 9.04\n",
      "iter-6000, loss 9.837\n",
      "iter-8000, loss 8.868\n",
      "iter-10000, loss 9.484\n",
      "iter-12000, loss 9.959\n",
      "iter-14000, loss 9.169\n",
      "Accuracy test =  16.476 %\n",
      "           0   1   2   3   4   5   6   7   8   9   10  11  12  13  14  15\n",
      "predicted  24  24  24  24  24  24  24  24  24  24  24  24  24  24  24  24\n",
      "truth      13  38  32   0   6  24  34  37  26  24  15  24  24  24  24  24\n",
      "24    60256\n",
      "dtype: int64\n",
      "Accuracy train =  16.224 %\n",
      "           0   1   2   3   4   5   6   7   8   9   10  11  12  13  14  15\n",
      "predicted  24  24  24  24  24  24  24  24  24  24  24  24  24  24  24  24\n",
      "truth      24  40  10  39  14   5   3   6  34  24  24  24  10  37  17  24\n",
      "24    140597\n",
      "dtype: int64\n",
      "printing learning rate after update:\n",
      "3.814697265625e-07\n",
      "\t +++ epoch: 18 +++\n",
      "iter-2000, loss 9.125\n",
      "iter-4000, loss 8.987\n",
      "iter-6000, loss 9.838\n",
      "iter-8000, loss 8.914\n",
      "iter-10000, loss 9.214\n",
      "iter-12000, loss 9.5\n",
      "iter-14000, loss 9.23\n",
      "Accuracy test =  16.476 %\n",
      "           0   1   2   3   4   5   6   7   8   9   10  11  12  13  14  15\n",
      "predicted  24  24  24  24  24  24  24  24  24  24  24  24  24  24  24  24\n",
      "truth      13  38  32   0   6  24  34  37  26  24  15  24  24  24  24  24\n",
      "24    60256\n",
      "dtype: int64\n",
      "Accuracy train =  16.224 %\n",
      "           0   1   2   3   4   5   6   7   8   9   10  11  12  13  14  15\n",
      "predicted  24  24  24  24  24  24  24  24  24  24  24  24  24  24  24  24\n",
      "truth      24  40  10  39  14   5   3   6  34  24  24  24  10  37  17  24\n",
      "24    140597\n",
      "dtype: int64\n",
      "printing learning rate after update:\n",
      "1.9073486328125e-07\n",
      "\t +++ epoch: 19 +++\n",
      "iter-2000, loss 9.057\n",
      "iter-4000, loss 9.729\n",
      "iter-6000, loss 9.653\n",
      "iter-8000, loss 9.505\n",
      "iter-10000, loss 9.315\n",
      "iter-12000, loss 9.774\n",
      "iter-14000, loss 9.005\n",
      "Accuracy test =  16.476 %\n",
      "           0   1   2   3   4   5   6   7   8   9   10  11  12  13  14  15\n",
      "predicted  24  24  24  24  24  24  24  24  24  24  24  24  24  24  24  24\n",
      "truth      13  38  32   0   6  24  34  37  26  24  15  24  24  24  24  24\n",
      "24    60256\n",
      "dtype: int64\n",
      "Accuracy train =  16.224 %\n",
      "           0   1   2   3   4   5   6   7   8   9   10  11  12  13  14  15\n",
      "predicted  24  24  24  24  24  24  24  24  24  24  24  24  24  24  24  24\n",
      "truth      24  40  10  39  14   5   3   6  34  24  24  24  10  37  17  24\n",
      "24    140597\n",
      "dtype: int64\n",
      "printing learning rate after update:\n",
      "9.5367431640625e-08\n",
      "\t +++ epoch: 20 +++\n",
      "iter-2000, loss 9.709\n",
      "iter-4000, loss 9.553\n",
      "iter-6000, loss 9.3\n",
      "iter-8000, loss 8.82\n",
      "iter-10000, loss 9.434\n",
      "iter-12000, loss 9.174\n",
      "iter-14000, loss 9.532\n",
      "Accuracy test =  16.476 %\n",
      "           0   1   2   3   4   5   6   7   8   9   10  11  12  13  14  15\n",
      "predicted  24  24  24  24  24  24  24  24  24  24  24  24  24  24  24  24\n",
      "truth      13  38  32   0   6  24  34  37  26  24  15  24  24  24  24  24\n",
      "24    60256\n",
      "dtype: int64\n",
      "Accuracy train =  16.224 %\n",
      "           0   1   2   3   4   5   6   7   8   9   10  11  12  13  14  15\n",
      "predicted  24  24  24  24  24  24  24  24  24  24  24  24  24  24  24  24\n",
      "truth      24  40  10  39  14   5   3   6  34  24  24  24  10  37  17  24\n",
      "24    140597\n",
      "dtype: int64\n",
      "printing learning rate after update:\n",
      "4.76837158203125e-08\n",
      "\t +++ epoch: 21 +++\n",
      "iter-2000, loss 9.511\n",
      "iter-4000, loss 9.391\n",
      "iter-6000, loss 9.032\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter-8000, loss 9.689\n",
      "iter-10000, loss 9.676\n",
      "iter-12000, loss 8.704\n",
      "iter-14000, loss 9.718\n",
      "Accuracy test =  16.476 %\n",
      "           0   1   2   3   4   5   6   7   8   9   10  11  12  13  14  15\n",
      "predicted  24  24  24  24  24  24  24  24  24  24  24  24  24  24  24  24\n",
      "truth      13  38  32   0   6  24  34  37  26  24  15  24  24  24  24  24\n",
      "24    60256\n",
      "dtype: int64\n",
      "Accuracy train =  16.224 %\n",
      "           0   1   2   3   4   5   6   7   8   9   10  11  12  13  14  15\n",
      "predicted  24  24  24  24  24  24  24  24  24  24  24  24  24  24  24  24\n",
      "truth      24  40  10  39  14   5   3   6  34  24  24  24  10  37  17  24\n",
      "24    140597\n",
      "dtype: int64\n",
      "printing learning rate after update:\n",
      "2.384185791015625e-08\n",
      "\t +++ epoch: 22 +++\n",
      "iter-2000, loss 9.665\n",
      "iter-4000, loss 9.257\n",
      "iter-6000, loss 9.334\n",
      "iter-8000, loss 9.047\n",
      "iter-10000, loss 9.663\n",
      "iter-12000, loss 9.675\n",
      "iter-14000, loss 9.844\n",
      "Accuracy test =  16.476 %\n",
      "           0   1   2   3   4   5   6   7   8   9   10  11  12  13  14  15\n",
      "predicted  24  24  24  24  24  24  24  24  24  24  24  24  24  24  24  24\n",
      "truth      13  38  32   0   6  24  34  37  26  24  15  24  24  24  24  24\n",
      "24    60256\n",
      "dtype: int64\n",
      "Accuracy train =  16.224 %\n",
      "           0   1   2   3   4   5   6   7   8   9   10  11  12  13  14  15\n",
      "predicted  24  24  24  24  24  24  24  24  24  24  24  24  24  24  24  24\n",
      "truth      24  40  10  39  14   5   3   6  34  24  24  24  10  37  17  24\n",
      "24    140597\n",
      "dtype: int64\n",
      "printing learning rate after update:\n",
      "1.1920928955078126e-08\n",
      "\t +++ epoch: 23 +++\n",
      "iter-2000, loss 9.083\n",
      "iter-4000, loss 9.812\n",
      "iter-6000, loss 8.627\n",
      "iter-8000, loss 9.305\n",
      "iter-10000, loss 9.151\n",
      "iter-12000, loss 9.736\n",
      "iter-14000, loss 8.798\n",
      "Accuracy test =  16.476 %\n",
      "           0   1   2   3   4   5   6   7   8   9   10  11  12  13  14  15\n",
      "predicted  24  24  24  24  24  24  24  24  24  24  24  24  24  24  24  24\n",
      "truth      13  38  32   0   6  24  34  37  26  24  15  24  24  24  24  24\n",
      "24    60256\n",
      "dtype: int64\n",
      "Accuracy train =  16.224 %\n",
      "           0   1   2   3   4   5   6   7   8   9   10  11  12  13  14  15\n",
      "predicted  24  24  24  24  24  24  24  24  24  24  24  24  24  24  24  24\n",
      "truth      24  40  10  39  14   5   3   6  34  24  24  24  10  37  17  24\n",
      "24    140597\n",
      "dtype: int64\n",
      "printing learning rate after update:\n",
      "5.960464477539063e-09\n",
      "\t +++ epoch: 24 +++\n",
      "iter-2000, loss 9.104\n",
      "iter-4000, loss 9.134\n",
      "iter-6000, loss 9.32\n",
      "iter-8000, loss 9.174\n",
      "iter-10000, loss 9.658\n",
      "iter-12000, loss 8.619\n",
      "iter-14000, loss 9.518\n",
      "Accuracy test =  16.476 %\n",
      "           0   1   2   3   4   5   6   7   8   9   10  11  12  13  14  15\n",
      "predicted  24  24  24  24  24  24  24  24  24  24  24  24  24  24  24  24\n",
      "truth      13  38  32   0   6  24  34  37  26  24  15  24  24  24  24  24\n",
      "24    60256\n",
      "dtype: int64\n",
      "Accuracy train =  16.224 %\n",
      "           0   1   2   3   4   5   6   7   8   9   10  11  12  13  14  15\n",
      "predicted  24  24  24  24  24  24  24  24  24  24  24  24  24  24  24  24\n",
      "truth      24  40  10  39  14   5   3   6  34  24  24  24  10  37  17  24\n",
      "24    140597\n",
      "dtype: int64\n",
      "printing learning rate after update:\n",
      "2.9802322387695314e-09\n",
      "\t +++ epoch: 25 +++\n",
      "iter-2000, loss 9.439\n",
      "iter-4000, loss 9.269\n",
      "iter-6000, loss 9.65\n",
      "iter-8000, loss 9.563\n",
      "iter-10000, loss 9.564\n"
     ]
    }
   ],
   "source": [
    "# train model using pytorch\n",
    "model = torch_net(X_train, Y_train.values,X_test,Y_test.values,[50],epoch=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find frequency of lemma per category\n",
    "tokens = matrix.get_feature_names()\n",
    "X_train= pd.DataFrame(X_train.todense(),columns=tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train['LABELED_CATEGORIES'] = categories\n",
    "X_gb = X_train.groupby('LABELED_CATEGORIES')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.25847805599662493"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.score(X_test,Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_gb.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_digits to', 'at n_digits', 'following', 'turned', 'we are', 'used', 'using', 'been', 'winning', 'running', 'better', 'n_digits people', 'n_digits thing', 'extreme', 'canada', 'asked', 'making', 'helping', 'waiting', 'creating', 'going', 'author', 'what is', 'helped', 'celebrating', 'horror', 'roy moore', 'higher', 'citizen', 'kid to', 'became', 'patrick', 'democracy', 'said', 'fighting', 'to n_digits', 'you are', 'is not', 'linked with', 'alleged', 'learned', 'lesbian', 'tuesday', 'mad', 'are you', 'dying', 'admits', 'trying', 'day n_digits', 'took', 'n_digits step', 'n_digits most', 'made', 'conversation with', 'shooting', 'for n_digits', 'lost n_digits', 'planned parenthood', 'continues', 'according to', 'chrissy', 'n_digits th', 'thing that', 'in n_digits', 'back at', 'is it', 'n_digits way', 'wa', 'dating', 'he wa', 'want you', 'losing', 'n_digits percent', 'here are', 'roundup of', 'lost', 'accused', 'loving', 'place to', 'too much', 'this is', 'saving', 'after n_digits', 'the n_digits', 'fall n_digits', 'n_digits of', 'staff', 'moving', 'woman who', 'n_digits and', 'better than', 'ebay', 'mile', 'linked', 'th', 'n_digits photo', 'theater', 'coming', 'killing', 'doing', 'syndrome', 'these n_digits', 'worse', 'easier', 'killed', 'funding', 'playing', 'keeping', 'calling', 'am', 'and n_digits', 'greatest', 'nomination', 'n_digits day', 'according', 'forever', 'working', 'people are', 'year after', 'becoming', 'charged', 'rose', 'thinking', 'watching', 'dressed', 'seriously', 'n_digits million', 'are not', 'accused of', 'raising', 'n_digits n_digits', 'people who', 'is the', 'growing', 'emergency', 'ending', 'than n_digits', 'based', 'amazing', 'is in', 'n_digits year', 'missing', 'from n_digits', 'saying', 'the worst', 'n_digits reason', 'learned from', 'n_digits new', 'called', 'taking', 'warning', 'weinstein', 'parenting', 'walking', 'it wa', 'did', 'that are', 'worst', 'least n_digits', 'did know', 'arrested', 'is still', 'getting', 'n_digits tip', 'got', 'trump is', 'musical', 'n_digits pound', 'changed', 'are', 'right to', 'agency', 'talking', 'teigen', 'fired', 'being', 'his own', 'tried', 'is what', 'over n_digits', 'insurance', 'feeling', 'barack', 'is an', 'season n_digits', 'had', 'were', 'lower', 'n_digits billion', 'eating', 'planned', 'ha', 'life of', 'having', 'seen', 'wearing', 'going to', 'wanted', 'n_digits easy', 'video of', 'divorced', 'doe', 'is', 'changing', 'happened', 'top n_digits', 'trying to', 'gave', 'done', 'is your', 'le', 'have been', 'n_digits best', 'went', 'learning', 'part n_digits', 'saved', 'with n_digits', 'n_digits minute', 'are the', 'stunning', 'field', 'it is', 'of n_digits', 'looking', 'breaking', 'decade', 'alive', 'biggest', 'rio', 'is so', 'latest', 'girlfriend', 'n_digits in', 'linked to', 'giving', 'n_digits time', 'finding', 'traveling', 'until', 'is just'}\n"
     ]
    }
   ],
   "source": [
    "tokens_01_no_digits = matrix.get_feature_names()\n",
    "print ( set(tokens_01_no_digits)-set(tokens_100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None,\n",
       "         normalize=False)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train model using linear regression\n",
    "\n",
    "linear_model   = sklearn.linear_model.LinearRegression()\n",
    "n = -1\n",
    "linear_model.fit(X_train[:n],Y_train[:n])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[38 34 10 ... 22 10 22]\n",
      "accuracy is 0.079668422\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.079668422"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_test = model.predict(X_test)\n",
    "n = 50000\n",
    "accuracy(p_test[:n],Y_test.values[:n])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "POLITICS          0.291940\n",
       "WELLNESS          0.237761\n",
       "ENTERTAINMENT     0.135637\n",
       "STYLE & BEAUTY    0.067890\n",
       "PARENTING         0.055722\n",
       "TRAVEL            0.043594\n",
       "CRIME             0.016360\n",
       "FOOD & DRINK      0.015892\n",
       "QUEER VOICES      0.015061\n",
       "WEDDINGS          0.013707\n",
       "HOME & LIVING     0.012800\n",
       "WOMEN             0.012522\n",
       "DIVORCE           0.010236\n",
       "COMEDY            0.009390\n",
       "SPORTS            0.009251\n",
       "BLACK VOICES      0.008389\n",
       "MEDIA             0.007000\n",
       "BUSINESS          0.006482\n",
       "THE WORLDPOST     0.004874\n",
       "GREEN             0.004122\n",
       "COLLEGE           0.003376\n",
       "TECH              0.002838\n",
       "ARTS              0.002405\n",
       "EDUCATION         0.001787\n",
       "WORLD NEWS        0.001509\n",
       "SCIENCE           0.001449\n",
       "PARENTS           0.001394\n",
       "IMPACT            0.000951\n",
       "WEIRD NEWS        0.000831\n",
       "ARTS & CULTURE    0.000772\n",
       "HEALTHY LIVING    0.000692\n",
       "WORLDPOST         0.000692\n",
       "RELIGION          0.000677\n",
       "MONEY             0.000632\n",
       "FIFTY             0.000553\n",
       "GOOD NEWS         0.000488\n",
       "STYLE             0.000239\n",
       "TASTE             0.000080\n",
       "ENVIRONMENT       0.000005\n",
       "dtype: float64"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "categories_names = Y.columns\n",
    "p_cat = pd.Series([categories_names[i] for i in p.argmax(axis = 1) ])\n",
    "p_cat.value_counts(normalize = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "POLITICS          0.163000\n",
       "WELLNESS          0.088756\n",
       "ENTERTAINMENT     0.079949\n",
       "TRAVEL            0.049225\n",
       "STYLE & BEAUTY    0.048040\n",
       "PARENTING         0.043201\n",
       "HEALTHY LIVING    0.033328\n",
       "QUEER VOICES      0.031436\n",
       "FOOD & DRINK      0.030998\n",
       "BUSINESS          0.029559\n",
       "COMEDY            0.025765\n",
       "SPORTS            0.024316\n",
       "BLACK VOICES      0.022544\n",
       "HOME & LIVING     0.020886\n",
       "PARENTS           0.019691\n",
       "THE WORLDPOST     0.018242\n",
       "WEDDINGS          0.018177\n",
       "WOMEN             0.017376\n",
       "IMPACT            0.017222\n",
       "DIVORCE           0.017057\n",
       "CRIME             0.016953\n",
       "MEDIA             0.014015\n",
       "WEIRD NEWS        0.013293\n",
       "GREEN             0.013054\n",
       "WORLDPOST         0.012840\n",
       "RELIGION          0.012726\n",
       "STYLE             0.011222\n",
       "SCIENCE           0.010844\n",
       "WORLD NEWS        0.010839\n",
       "TASTE             0.010435\n",
       "TECH              0.010366\n",
       "MONEY             0.008499\n",
       "ARTS              0.007513\n",
       "FIFTY             0.006975\n",
       "GOOD NEWS         0.006960\n",
       "ARTS & CULTURE    0.006667\n",
       "ENVIRONMENT       0.006587\n",
       "COLLEGE           0.005696\n",
       "LATINO VOICES     0.005621\n",
       "CULTURE & ARTS    0.005128\n",
       "EDUCATION         0.004999\n",
       "Name: category, dtype: float64"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.category.value_counts(normalize = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fit complete\n"
     ]
    }
   ],
   "source": [
    "# KNN\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "neigh = KNeighborsClassifier(n_neighbors=3)\n",
    "neigh.fit(X_train, cat_train) \n",
    "print (\"fit complete\")\n",
    "# print (neigh.score(X_test,cat_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<class 'numpy.ndarray'>, <class 'pandas.core.series.Series'>, <class 'numpy.ndarray'>]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>POLITICS</td>\n",
       "      <td>WOMEN</td>\n",
       "      <td>how rape culture and racism combine to hurt as...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>BUSINESS</td>\n",
       "      <td>TRAVEL</td>\n",
       "      <td>turkey season travel deal be all stuff and no ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>QUEER VOICES</td>\n",
       "      <td>HOME &amp; LIVING</td>\n",
       "      <td>this oscar season , we 're see red</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>HEALTHY LIVING</td>\n",
       "      <td>HEALTHY LIVING</td>\n",
       "      <td>for target cancer treatment , just upload your...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ENTERTAINMENT</td>\n",
       "      <td>PARENTS</td>\n",
       "      <td>13 must-reads for blend family</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>FOOD &amp; DRINK</td>\n",
       "      <td>RELIGION</td>\n",
       "      <td>god and the battle over woman 's body ( all to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>PARENTS</td>\n",
       "      <td>WELLNESS</td>\n",
       "      <td>acupuncturists spill : the 12 health tip they ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>COMEDY</td>\n",
       "      <td>WEDDINGS</td>\n",
       "      <td>drunk groom caught cheat on his bride during r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>FOOD &amp; DRINK</td>\n",
       "      <td>BUSINESS</td>\n",
       "      <td>welfare limit left poor adrift a recession hit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>FOOD &amp; DRINK</td>\n",
       "      <td>ENTERTAINMENT</td>\n",
       "      <td>tom hank will play mr. rogers in 'you be my fr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>FIFTY</td>\n",
       "      <td>ARTS</td>\n",
       "      <td>once a soldier , today a heartist . the journe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>FOOD &amp; DRINK</td>\n",
       "      <td>WOMEN</td>\n",
       "      <td>marcia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>BLACK VOICES</td>\n",
       "      <td>WEDDINGS</td>\n",
       "      <td>turn volunteer into wedding planning peace</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>BLACK VOICES</td>\n",
       "      <td>ENTERTAINMENT</td>\n",
       "      <td>natalie portman a jackie kennedy : 'black swan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>ENTERTAINMENT</td>\n",
       "      <td>ENTERTAINMENT</td>\n",
       "      <td>'warm body ' lead the weekend box office with ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>WELLNESS</td>\n",
       "      <td>PARENTS</td>\n",
       "      <td>10 autism lesson you will not find in a textbook</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>POLITICS</td>\n",
       "      <td>POLITICS</td>\n",
       "      <td>jeb bush : i , too , would have authorize iraq...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>WELLNESS</td>\n",
       "      <td>ENTERTAINMENT</td>\n",
       "      <td>aaron sorkin partly blame medium for 'the inte...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>WELLNESS</td>\n",
       "      <td>WELLNESS</td>\n",
       "      <td>i lose weight : lindsay spaulding learn about ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>TRAVEL</td>\n",
       "      <td>TRAVEL</td>\n",
       "      <td>best large-ship cruise line ( photo )</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>CULTURE &amp; ARTS</td>\n",
       "      <td>WELLNESS</td>\n",
       "      <td>i 'd rather punch myself in the neck than meet...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>BUSINESS</td>\n",
       "      <td>POLITICS</td>\n",
       "      <td>why canada be under attack by terrorist</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>BLACK VOICES</td>\n",
       "      <td>WEDDINGS</td>\n",
       "      <td>wedding trailblazer : 'the artful bachelorette...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>FOOD &amp; DRINK</td>\n",
       "      <td>POLITICS</td>\n",
       "      <td>gop lawmaker will soon be allow to review clas...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>FOOD &amp; DRINK</td>\n",
       "      <td>FOOD &amp; DRINK</td>\n",
       "      <td>the 5 indispensable grill tool to get for fath...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>FOOD &amp; DRINK</td>\n",
       "      <td>FOOD &amp; DRINK</td>\n",
       "      <td>top 5 taste : i 'm back and i 'm break bread i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>DIVORCE</td>\n",
       "      <td>PARENTS</td>\n",
       "      <td>43 life lesson i share with my daughter</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>PARENTING</td>\n",
       "      <td>PARENTS</td>\n",
       "      <td>a mom speaks out against 'mom culture '</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>PARENTING</td>\n",
       "      <td>POLITICS</td>\n",
       "      <td>melania trump be forge her own path a first la...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>POLITICS</td>\n",
       "      <td>POLITICS</td>\n",
       "      <td>hillary clinton pledge to take on immigration ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>COMEDY</td>\n",
       "      <td>COMEDY</td>\n",
       "      <td>the trouble with burrito</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>FOOD &amp; DRINK</td>\n",
       "      <td>HOME &amp; LIVING</td>\n",
       "      <td>hot in the kitchen : the coolest cooking produ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>WELLNESS</td>\n",
       "      <td>SPORTS</td>\n",
       "      <td>derek jeter cornfield maze will stalk your dream</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>MEDIA</td>\n",
       "      <td>SPORTS</td>\n",
       "      <td>hope solo advocate for equal pay for female at...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>PARENTING</td>\n",
       "      <td>ENTERTAINMENT</td>\n",
       "      <td>ne-yo on 'saturday night live ' : singer perfo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>IMPACT</td>\n",
       "      <td>DIVORCE</td>\n",
       "      <td>a gift from a parent to a child</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>DIVORCE</td>\n",
       "      <td>COMEDY</td>\n",
       "      <td>where 's my 'responsible enough to not have a ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>FOOD &amp; DRINK</td>\n",
       "      <td>PARENTING</td>\n",
       "      <td>the power of the pregnant body</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>POLITICS</td>\n",
       "      <td>BUSINESS</td>\n",
       "      <td>amazon 's jeff bezos be still the world 's bes...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>FOOD &amp; DRINK</td>\n",
       "      <td>FOOD &amp; DRINK</td>\n",
       "      <td>starbucks will soon sell booze in thousand of ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>PARENTING</td>\n",
       "      <td>SPORTS</td>\n",
       "      <td>103-year-old golfer most likely break record w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>WORLDPOST</td>\n",
       "      <td>WORLDPOST</td>\n",
       "      <td>u.s. and nato should end new cold war with russia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>ENTERTAINMENT</td>\n",
       "      <td>BUSINESS</td>\n",
       "      <td>paul krugman : 'yes , brexit will make britain...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>COMEDY</td>\n",
       "      <td>TRAVEL</td>\n",
       "      <td>new york 's best new restaurant ( photo )</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>COMEDY</td>\n",
       "      <td>POLITICS</td>\n",
       "      <td>'death to america ' : shia-sunni rivalry and t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>BUSINESS</td>\n",
       "      <td>WOMEN</td>\n",
       "      <td>step into an executive role : the thing i knew...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>PARENTS</td>\n",
       "      <td>WOMEN</td>\n",
       "      <td>amber riley to body-shamers : 'let my big a** ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>POLITICS</td>\n",
       "      <td>POLITICS</td>\n",
       "      <td>sen. mitch mcconnell defends hypocrisy on sena...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>POLITICS</td>\n",
       "      <td>POLITICS</td>\n",
       "      <td>trump face his 'mccarthy moment ' -- why do it...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>FOOD &amp; DRINK</td>\n",
       "      <td>FOOD &amp; DRINK</td>\n",
       "      <td>flat pack cheese grater</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>SCIENCE</td>\n",
       "      <td>TRAVEL</td>\n",
       "      <td>kashmir 's peaceful autumn ( photo )</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>GREEN</td>\n",
       "      <td>RELIGION</td>\n",
       "      <td>chef roast diner who left note call christmas ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>SCIENCE</td>\n",
       "      <td>HEALTHY LIVING</td>\n",
       "      <td>workaholic may be at great risk for these psyc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>QUEER VOICES</td>\n",
       "      <td>QUEER VOICES</td>\n",
       "      <td>pee while trans*</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>WELLNESS</td>\n",
       "      <td>PARENTING</td>\n",
       "      <td>parent inside out : 4 principle to create peace</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>FOOD &amp; DRINK</td>\n",
       "      <td>POLITICS</td>\n",
       "      <td>democrat go after jimmy john 's and amazon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>ARTS</td>\n",
       "      <td>COMEDY</td>\n",
       "      <td>conan o'brien review 'michael phelps : push th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>POLITICS</td>\n",
       "      <td>POLITICS</td>\n",
       "      <td>how view on gun have change since the parkland...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>FOOD &amp; DRINK</td>\n",
       "      <td>ENTERTAINMENT</td>\n",
       "      <td>fetch yourself some nostalgia with this clip o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>DIVORCE</td>\n",
       "      <td>STYLE</td>\n",
       "      <td>10 sign you 're the 'stylish one ' in your gro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 0               1  \\\n",
       "0         POLITICS           WOMEN   \n",
       "1         BUSINESS          TRAVEL   \n",
       "2     QUEER VOICES   HOME & LIVING   \n",
       "3   HEALTHY LIVING  HEALTHY LIVING   \n",
       "4    ENTERTAINMENT         PARENTS   \n",
       "5     FOOD & DRINK        RELIGION   \n",
       "6          PARENTS        WELLNESS   \n",
       "7           COMEDY        WEDDINGS   \n",
       "8     FOOD & DRINK        BUSINESS   \n",
       "9     FOOD & DRINK   ENTERTAINMENT   \n",
       "10           FIFTY            ARTS   \n",
       "11    FOOD & DRINK           WOMEN   \n",
       "12    BLACK VOICES        WEDDINGS   \n",
       "13    BLACK VOICES   ENTERTAINMENT   \n",
       "14   ENTERTAINMENT   ENTERTAINMENT   \n",
       "15        WELLNESS         PARENTS   \n",
       "16        POLITICS        POLITICS   \n",
       "17        WELLNESS   ENTERTAINMENT   \n",
       "18        WELLNESS        WELLNESS   \n",
       "19          TRAVEL          TRAVEL   \n",
       "20  CULTURE & ARTS        WELLNESS   \n",
       "21        BUSINESS        POLITICS   \n",
       "22    BLACK VOICES        WEDDINGS   \n",
       "23    FOOD & DRINK        POLITICS   \n",
       "24    FOOD & DRINK    FOOD & DRINK   \n",
       "25    FOOD & DRINK    FOOD & DRINK   \n",
       "26         DIVORCE         PARENTS   \n",
       "27       PARENTING         PARENTS   \n",
       "28       PARENTING        POLITICS   \n",
       "29        POLITICS        POLITICS   \n",
       "..             ...             ...   \n",
       "70          COMEDY          COMEDY   \n",
       "71    FOOD & DRINK   HOME & LIVING   \n",
       "72        WELLNESS          SPORTS   \n",
       "73           MEDIA          SPORTS   \n",
       "74       PARENTING   ENTERTAINMENT   \n",
       "75          IMPACT         DIVORCE   \n",
       "76         DIVORCE          COMEDY   \n",
       "77    FOOD & DRINK       PARENTING   \n",
       "78        POLITICS        BUSINESS   \n",
       "79    FOOD & DRINK    FOOD & DRINK   \n",
       "80       PARENTING          SPORTS   \n",
       "81       WORLDPOST       WORLDPOST   \n",
       "82   ENTERTAINMENT        BUSINESS   \n",
       "83          COMEDY          TRAVEL   \n",
       "84          COMEDY        POLITICS   \n",
       "85        BUSINESS           WOMEN   \n",
       "86         PARENTS           WOMEN   \n",
       "87        POLITICS        POLITICS   \n",
       "88        POLITICS        POLITICS   \n",
       "89    FOOD & DRINK    FOOD & DRINK   \n",
       "90         SCIENCE          TRAVEL   \n",
       "91           GREEN        RELIGION   \n",
       "92         SCIENCE  HEALTHY LIVING   \n",
       "93    QUEER VOICES    QUEER VOICES   \n",
       "94        WELLNESS       PARENTING   \n",
       "95    FOOD & DRINK        POLITICS   \n",
       "96            ARTS          COMEDY   \n",
       "97        POLITICS        POLITICS   \n",
       "98    FOOD & DRINK   ENTERTAINMENT   \n",
       "99         DIVORCE           STYLE   \n",
       "\n",
       "                                                    2  \n",
       "0   how rape culture and racism combine to hurt as...  \n",
       "1   turkey season travel deal be all stuff and no ...  \n",
       "2                  this oscar season , we 're see red  \n",
       "3   for target cancer treatment , just upload your...  \n",
       "4                      13 must-reads for blend family  \n",
       "5   god and the battle over woman 's body ( all to...  \n",
       "6   acupuncturists spill : the 12 health tip they ...  \n",
       "7   drunk groom caught cheat on his bride during r...  \n",
       "8      welfare limit left poor adrift a recession hit  \n",
       "9   tom hank will play mr. rogers in 'you be my fr...  \n",
       "10  once a soldier , today a heartist . the journe...  \n",
       "11                                             marcia  \n",
       "12         turn volunteer into wedding planning peace  \n",
       "13  natalie portman a jackie kennedy : 'black swan...  \n",
       "14  'warm body ' lead the weekend box office with ...  \n",
       "15   10 autism lesson you will not find in a textbook  \n",
       "16  jeb bush : i , too , would have authorize iraq...  \n",
       "17  aaron sorkin partly blame medium for 'the inte...  \n",
       "18  i lose weight : lindsay spaulding learn about ...  \n",
       "19              best large-ship cruise line ( photo )  \n",
       "20  i 'd rather punch myself in the neck than meet...  \n",
       "21            why canada be under attack by terrorist  \n",
       "22  wedding trailblazer : 'the artful bachelorette...  \n",
       "23  gop lawmaker will soon be allow to review clas...  \n",
       "24  the 5 indispensable grill tool to get for fath...  \n",
       "25  top 5 taste : i 'm back and i 'm break bread i...  \n",
       "26            43 life lesson i share with my daughter  \n",
       "27            a mom speaks out against 'mom culture '  \n",
       "28  melania trump be forge her own path a first la...  \n",
       "29  hillary clinton pledge to take on immigration ...  \n",
       "..                                                ...  \n",
       "70                           the trouble with burrito  \n",
       "71  hot in the kitchen : the coolest cooking produ...  \n",
       "72   derek jeter cornfield maze will stalk your dream  \n",
       "73  hope solo advocate for equal pay for female at...  \n",
       "74  ne-yo on 'saturday night live ' : singer perfo...  \n",
       "75                    a gift from a parent to a child  \n",
       "76  where 's my 'responsible enough to not have a ...  \n",
       "77                     the power of the pregnant body  \n",
       "78  amazon 's jeff bezos be still the world 's bes...  \n",
       "79  starbucks will soon sell booze in thousand of ...  \n",
       "80  103-year-old golfer most likely break record w...  \n",
       "81  u.s. and nato should end new cold war with russia  \n",
       "82  paul krugman : 'yes , brexit will make britain...  \n",
       "83          new york 's best new restaurant ( photo )  \n",
       "84  'death to america ' : shia-sunni rivalry and t...  \n",
       "85  step into an executive role : the thing i knew...  \n",
       "86  amber riley to body-shamers : 'let my big a** ...  \n",
       "87  sen. mitch mcconnell defends hypocrisy on sena...  \n",
       "88  trump face his 'mccarthy moment ' -- why do it...  \n",
       "89                            flat pack cheese grater  \n",
       "90               kashmir 's peaceful autumn ( photo )  \n",
       "91  chef roast diner who left note call christmas ...  \n",
       "92  workaholic may be at great risk for these psyc...  \n",
       "93                                   pee while trans*  \n",
       "94    parent inside out : 4 principle to create peace  \n",
       "95         democrat go after jimmy john 's and amazon  \n",
       "96  conan o'brien review 'michael phelps : push th...  \n",
       "97  how view on gun have change since the parkland...  \n",
       "98  fetch yourself some nostalgia with this clip o...  \n",
       "99  10 sign you 're the 'stylish one ' in your gro...  \n",
       "\n",
       "[100 rows x 3 columns]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n = 100\n",
    "print ([type(t) for t in (neigh.predict(X_test[:n]),cat_test[:n],np.array(headlines_test[:n]))])\n",
    "# print ((neigh.predict(X_test[:n]),cat_test[:n], np.array(headlines_test[:n])))\n",
    "pd.DataFrame( (neigh.predict(X_test[:n]),cat_test[:n], np.array(headlines_test[:n]))).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
